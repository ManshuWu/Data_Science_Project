{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/fullfull/annotations\")\n",
    "OUTPUT_FILE = Path(\"merged_spans_with_entities.jsonl\")\n",
    "\n",
    "merged = []\n",
    "\n",
    "for span_path in sorted(INPUT_DIR.glob(\"*_spans.jsonl\")):\n",
    "    filename = span_path.name\n",
    "    with span_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for lineno, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                print(f\"Skipping empty line at {filename}:{lineno}\")\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error at {filename}:{lineno} — {e}\")\n",
    "                continue\n",
    "\n",
    "            spans = rec.get(\"spans\", [])\n",
    "            if not spans:\n",
    "                continue\n",
    "\n",
    "            entry = {\n",
    "                \"text\": rec.get(\"text\", \"\"),\n",
    "                \"tokens\": rec.get(\"tokens\", []),\n",
    "                \"spans\": spans,\n",
    "            }\n",
    "            merged.append(entry)\n",
    "\n",
    "with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as fw:\n",
    "    for entry in merged:\n",
    "        fw.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Merged and saved {len(merged)} entity-containing records to: {OUTPUT_FILE.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_DIR = Path(\"/kaggle/input/fullfull/annotations\")      \n",
    "OUTPUT_FILE = Path(\"merged_spans.jsonl\")                 \n",
    "\n",
    "merged = []\n",
    "\n",
    "for span_path in sorted(INPUT_DIR.glob(\"*_spans.jsonl\")):\n",
    "    filename = span_path.name\n",
    "    with span_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for lineno, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                print(f\"Skipping empty line at {filename}:{lineno}\")\n",
    "                continue\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error at {filename}:{lineno} — {e}\")\n",
    "                continue\n",
    "\n",
    "            spans = rec.get(\"spans\", [])\n",
    "            if not spans:\n",
    "                continue\n",
    "\n",
    "            merged.append({\n",
    "                \"text\": rec.get(\"text\", \"\"),\n",
    "                \"spans\": spans,\n",
    "            })\n",
    "\n",
    "with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as fw:\n",
    "    for entry in merged:\n",
    "        fw.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Merged and saved {len(merged)} records (text + spans) to: {OUTPUT_FILE.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "INPUT_XLSX = Path(\"/kaggle/input/polgtable/subset_POLG.xlsx\")\n",
    "OUTPUT_FILE = Path(\"hpo_terms.jsonl\")\n",
    "\n",
    "df = pd.read_excel(INPUT_XLSX, engine=\"openpyxl\")\n",
    "\n",
    "\n",
    "all_hpo_terms = []\n",
    "seen = set()\n",
    "\n",
    "for terms in df[\"HPO_Term\"]:\n",
    "    if pd.isna(terms):\n",
    "        continue\n",
    "    for term in str(terms).split(\";\"):\n",
    "        term = term.strip()\n",
    "        if term and term not in seen:\n",
    "            all_hpo_terms.append(term)\n",
    "            seen.add(term)\n",
    "\n",
    "with OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for hpo in all_hpo_terms:\n",
    "        fout.write(json.dumps({\"HPO_TERM\": hpo}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"{len(all_hpo_terms)}  {OUTPUT_FILE.resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# -------------------\n",
    "# Constants & Paths\n",
    "# -------------------\n",
    "FILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\n",
    "OUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_BIO = OUT_DIR / \"train.jsonl\"\n",
    "DEV_BIO   = OUT_DIR / \"dev.jsonl\"\n",
    "TEST_BIO  = OUT_DIR / \"test.jsonl\"\n",
    "TEST_TEXT_ONLY = OUT_DIR / \"test_text_only.jsonl\"  \n",
    "\n",
    "ENTITY_TYPES = {\n",
    "    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n",
    "    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# Utility Functions\n",
    "# -------------------\n",
    "def iter_jsonl(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "def make_bio_labels(spans, enc):\n",
    "    tokens   = enc.tokens()\n",
    "    offsets  = enc[\"offset_mapping\"]\n",
    "    word_ids = enc.word_ids()\n",
    "    tags     = [\"O\"] * len(tokens)\n",
    "    span_to_tokens = []\n",
    "\n",
    "    for sp in spans:\n",
    "        s, e, typ = sp[\"start\"], sp[\"end\"], sp[\"label\"]\n",
    "        idxs = [\n",
    "            i for i, (b, t) in enumerate(offsets)\n",
    "            if not (t <= s or b >= e)\n",
    "        ]\n",
    "        span_to_tokens.append(idxs)\n",
    "        if not idxs:\n",
    "            continue\n",
    "        tags[idxs[0]] = f\"B-{typ}\"\n",
    "        for i in idxs[1:]:\n",
    "            tags[i] = f\"I-{typ}\"\n",
    "\n",
    "    for idxs in span_to_tokens:\n",
    "        if len(idxs) == 1:\n",
    "            tags[idxs[0]] = tags[idxs[0]].replace(\"I-\", \"B-\")\n",
    "\n",
    "    prev_wid = None\n",
    "    for i, wid in enumerate(word_ids):\n",
    "        if (wid is not None and wid == prev_wid\n",
    "                and tags[i] == \"O\"\n",
    "                and tags[i - 1].startswith((\"B-\", \"I-\"))):\n",
    "            tags[i] = \"I-\" + tags[i - 1][2:]\n",
    "        prev_wid = wid\n",
    "\n",
    "    return tags\n",
    "\n",
    "def record_to_bio(rec):\n",
    "    text  = rec.get(\"text\", \"\")\n",
    "    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n",
    "    if not spans:\n",
    "        return None\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return {\n",
    "        \"text\": text,  \n",
    "        \"tokens\": enc.tokens(),\n",
    "        \"labels\": make_bio_labels(spans, enc)\n",
    "    }\n",
    "\n",
    "def dump_jsonl(path: Path, data):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for obj in data:\n",
    "            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# -------------------\n",
    "# Load & Convert\n",
    "# -------------------\n",
    "print(\">> Loading and converting gold-standard data …\")\n",
    "merged_bio = [\n",
    "    bio for rec in iter_jsonl(FILE_MERGED)\n",
    "    if (bio := record_to_bio(rec)) is not None\n",
    "]\n",
    "print(f\"Total valid records: {len(merged_bio)}\")\n",
    "\n",
    "# -------------------\n",
    "# Simple Random Split (70/20/10)\n",
    "# -------------------\n",
    "train_set, temp_set = train_test_split(\n",
    "    merged_bio, test_size=0.3, random_state=42\n",
    ")\n",
    "dev_set, test_set = train_test_split(\n",
    "    temp_set, test_size=1/3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n",
    "\n",
    "# -------------------\n",
    "# Save BIO Format Files\n",
    "# -------------------\n",
    "dump_jsonl(TRAIN_BIO, train_set)\n",
    "dump_jsonl(DEV_BIO, dev_set)\n",
    "dump_jsonl(TEST_BIO, test_set)\n",
    "\n",
    "# -------------------\n",
    "# Save only raw test set texts\n",
    "# -------------------\n",
    "with TEST_TEXT_ONLY.open(\"w\", encoding=\"utf-8\") as fw:\n",
    "    for ex in test_set:\n",
    "        if \"text\" in ex:\n",
    "            fw.write(json.dumps({\"text\": ex[\"text\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved to {TRAIN_BIO}, {DEV_BIO}, {TEST_BIO}\")\n",
    "print(f\" Raw test text saved to: {TEST_TEXT_ONLY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install seqeval evaluate torchcrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# 1. Load BIO datasets\n",
    "BIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def keep_only_hpo_labels(example):\n",
    "    example[\"labels\"] = [\n",
    "        lab if lab.endswith(\"HPO_TERM\") else \"O\"\n",
    "        for lab in example[\"labels\"]\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "train_examples = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"train.jsonl\")]\n",
    "dev_examples   = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"dev.jsonl\")]\n",
    "test_examples  = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"test.jsonl\")]\n",
    "\n",
    "ds_splits = DatasetDict({\n",
    "    \"train\":      Dataset.from_list(train_examples),\n",
    "    \"validation\": Dataset.from_list(dev_examples),\n",
    "    \"test\":       Dataset.from_list(test_examples),\n",
    "})\n",
    "print(\"Loaded dataset sizes:\", {k: len(v) for k, v in ds_splits.items()})\n",
    "\n",
    "# 2. Tokenizer & label mapping\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "# 2.1['B-HPO_TERM', 'I-HPO_TERM', 'O']\n",
    "unique_labels = sorted({lab for ex in train_examples + dev_examples + test_examples\n",
    "                        for lab in ex[\"labels\"]})\n",
    "label2id = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "def tokenize_and_align_labels(ex):\n",
    "    enc = tokenizer(\n",
    "        ex[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    enc[\"labels\"] = [label2id[l] for l in ex[\"labels\"]]\n",
    "    return enc\n",
    "\n",
    "ds_splits = ds_splits.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=[\"tokens\", \"labels\"],\n",
    ")\n",
    "\n",
    "# 3. Model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# 4. Metrics\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    refs = p.label_ids\n",
    "    true_labels = [\n",
    "        [id2label[lid] for lid in seq if lid != -100] for seq in refs\n",
    "    ]\n",
    "    pred_labels = [\n",
    "        [id2label[pid] for pid, lid in zip(pred_seq, ref_seq) if lid != -100]\n",
    "        for pred_seq, ref_seq in zip(preds, refs)\n",
    "    ]\n",
    "    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "    return {\n",
    "        \"overall_precision\": result[\"overall_precision\"],\n",
    "        \"overall_recall\":    result[\"overall_recall\"],\n",
    "        \"overall_f1\":        result[\"overall_f1\"],\n",
    "        \"overall_accuracy\":  result[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# 5. Training arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner_pubmedbert\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[\"none\"],\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_splits[\"train\"],\n",
    "    eval_dataset=ds_splits[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 6. Train and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# 7. Predict on test set\n",
    "test_metrics = trainer.predict(ds_splits[\"test\"]).metrics\n",
    "print(\"Test set metrics:\", test_metrics)\n",
    "predictions, labels, _ = trainer.predict(ds_splits[\"test\"])\n",
    "preds = predictions.argmax(-1)\n",
    "\n",
    "true_labels = [\n",
    "    [id2label[label_id] for label_id in seq if label_id != -100]\n",
    "    for seq in labels\n",
    "]\n",
    "pred_labels = [\n",
    "    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n",
    "    for pred_seq, label_seq in zip(preds, labels)\n",
    "]\n",
    "\n",
    "detailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "\n",
    "# 8. Only show HPO_TERM in per-label report\n",
    "print(\"\\n HPO_TERM classification report:\")\n",
    "for label, metrics in detailed_result.items():\n",
    "    if label.startswith(\"overall_\"):\n",
    "        continue\n",
    "    if label != \"HPO_TERM\":\n",
    "        continue\n",
    "    print(f\" {label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "\n",
    "predictions, labels, _ = trainer.predict(ds_splits[\"test\"])\n",
    "preds = predictions.argmax(-1)\n",
    "\n",
    "true_labels = [\n",
    "    [id2label[label_id] for label_id in seq if label_id != -100]\n",
    "    for seq in labels\n",
    "]\n",
    "pred_labels = [\n",
    "    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n",
    "    for pred_seq, label_seq in zip(preds, labels)\n",
    "]\n",
    "\n",
    "\n",
    "seqeval = load(\"seqeval\")\n",
    "detailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "\n",
    "\n",
    "print(f\"\\nOverall F1 score: {detailed_result.get('overall_f1', 0):.3f}\")\n",
    "\n",
    "print(\"\\n HPO_TERM classification report:\")\n",
    "hpo_metrics = detailed_result.get(\"HPO_TERM\")\n",
    "if hpo_metrics:\n",
    "    print(f\" {'HPO_TERM':20} | Precision: {hpo_metrics['precision']:.3f} | Recall: {hpo_metrics['recall']:.3f} | F1: {hpo_metrics['f1']:.3f}\")\n",
    "else:\n",
    "    print(\"No HPO_TERM entities found in predictions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_entities(labels):\n",
    "    spans = []\n",
    "    start = None\n",
    "    current_label = None\n",
    "    for i, lab_id in enumerate(labels):\n",
    "        label = id2label.get(lab_id, \"O\")\n",
    "        if label.startswith(\"B-HPO_TERM\"):\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "            start = i\n",
    "            current_label = \"HPO_TERM\"\n",
    "        elif label.startswith(\"I-HPO_TERM\") and current_label:\n",
    "            continue\n",
    "        else:\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "                current_label = None\n",
    "                start = None\n",
    "    if current_label:\n",
    "        spans.append((start, len(labels) - 1, current_label))\n",
    "    return spans\n",
    "\n",
    "def iou(a, b):\n",
    "    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n",
    "    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n",
    "    return inter / union\n",
    "\n",
    "def relaxed_match(pred_span, true_span):\n",
    "    ps, pe, plabel = pred_span\n",
    "    ts, te, tlabel = true_span\n",
    "    if plabel != tlabel:\n",
    "        return False\n",
    "    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n",
    "        return True\n",
    "    if iou((ps, pe), (ts, te)) >= 0.4:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def relaxed_compute_metrics(preds, refs):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "\n",
    "    for pred_seq, ref_seq in zip(preds, refs):\n",
    "        pred_ents = extract_entities(pred_seq)\n",
    "        true_ents = extract_entities(ref_seq)\n",
    "        matched = set()\n",
    "\n",
    "        for pred_ent in pred_ents:\n",
    "            match_found = False\n",
    "            for i, true_ent in enumerate(true_ents):\n",
    "                if i in matched:\n",
    "                    continue\n",
    "                if relaxed_match(pred_ent, true_ent):\n",
    "                    tp += 1\n",
    "                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n",
    "                    matched.add(i)\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if not match_found:\n",
    "                fp += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n",
    "\n",
    "        for i, true_ent in enumerate(true_ents):\n",
    "            if i not in matched:\n",
    "                fn += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall    = tp / (tp + fn + 1e-10)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    print(\"\\n Relaxed Per-label HPO_TERM classification report:\")\n",
    "    for label, m in label_metrics.items():\n",
    "        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n",
    "        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n",
    "        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n",
    "        print(f\" {label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "filtered_preds = []\n",
    "filtered_labels = []\n",
    "\n",
    "for pred_seq, label_seq in zip(preds, labels):\n",
    "    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n",
    "    filtered_label = [l for l in label_seq if l != -100]\n",
    "    filtered_preds.append(filtered_pred)\n",
    "    filtered_labels.append(filtered_label)\n",
    "\n",
    "def clean_prediction_structure(labels):\n",
    "\n",
    "    cleaned = []\n",
    "    prev = \"O\"\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.startswith(\"I-\") and prev == \"O\":\n",
    "            label = \"B-\" + label[2:]\n",
    "        if label == \"O\" and i+2 < len(labels) and labels[i+1].startswith(\"B-\") and labels[i+2].startswith(\"I-\"):\n",
    "            label = \"I-\" + labels[i+1][2:]\n",
    "        cleaned.append(label)\n",
    "        prev = label\n",
    "    return cleaned\n",
    "\n",
    "def fix_illegal_I(labels):\n",
    "\n",
    "    fixed = []\n",
    "    prev_type = \"O\"\n",
    "    for label in labels:\n",
    "        if label.startswith(\"I-\"):\n",
    "            if prev_type != label[2:]:\n",
    "                label = \"B-\" + label[2:]\n",
    "        fixed.append(label)\n",
    "        if label.startswith(\"B-\"):\n",
    "            prev_type = label[2:]\n",
    "        elif label.startswith(\"I-\"):\n",
    "            pass\n",
    "        else:\n",
    "            prev_type = \"O\"\n",
    "    return fixed\n",
    "\n",
    "def clean_and_fix_prediction_sequence(label_ids):\n",
    "\n",
    "    labels = [id2label.get(lid, \"O\") for lid in label_ids]\n",
    "    labels = clean_prediction_structure(labels)\n",
    "    labels = fix_illegal_I(labels)\n",
    "    return [label2id.get(l, 0) for l in labels]\n",
    "\n",
    "\n",
    "filtered_preds_cleaned = [clean_and_fix_prediction_sequence(seq) for seq in filtered_preds]\n",
    "\n",
    "print(\"\\n Running relaxed evaluation on test set (HPO_TERM only, with structure repair)...\")\n",
    "relaxed_metrics = relaxed_compute_metrics(filtered_preds_cleaned, filtered_labels)\n",
    "print(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_entities(labels):\n",
    "    spans = []\n",
    "    start = None\n",
    "    current_label = None\n",
    "    for i, lab_id in enumerate(labels):\n",
    "        label = id2label.get(lab_id, \"O\")\n",
    "        if label.startswith(\"B-HPO_TERM\"):\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "            start = i\n",
    "            current_label = \"HPO_TERM\"\n",
    "        elif label.startswith(\"I-HPO_TERM\") and current_label:\n",
    "            continue\n",
    "        else:\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "                current_label = None\n",
    "                start = None\n",
    "    if current_label:\n",
    "        spans.append((start, len(labels) - 1, current_label))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n",
    "    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n",
    "    return inter / union\n",
    "\n",
    "def relaxed_match(pred_span, true_span):\n",
    "    ps, pe, plabel = pred_span\n",
    "    ts, te, tlabel = true_span\n",
    "    if plabel != tlabel:\n",
    "        return False\n",
    "    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n",
    "        return True\n",
    "    if iou((ps, pe), (ts, te)) >= 0.4:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def relaxed_compute_metrics(preds, refs):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "\n",
    "    for pred_seq, ref_seq in zip(preds, refs):\n",
    "        pred_ents = extract_entities(pred_seq)\n",
    "        true_ents = extract_entities(ref_seq)\n",
    "        matched = set()\n",
    "\n",
    "        for pred_ent in pred_ents:\n",
    "            match_found = False\n",
    "            for i, true_ent in enumerate(true_ents):\n",
    "                if i in matched:\n",
    "                    continue\n",
    "                if relaxed_match(pred_ent, true_ent):\n",
    "                    tp += 1\n",
    "                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n",
    "                    matched.add(i)\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if not match_found:\n",
    "                fp += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n",
    "\n",
    "        for i, true_ent in enumerate(true_ents):\n",
    "            if i not in matched:\n",
    "                fn += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall    = tp / (tp + fn + 1e-10)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    print(\"\\n Relaxed Per-label HPO_TERM classification report:\")\n",
    "    for label, m in label_metrics.items():\n",
    "        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n",
    "        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n",
    "        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n",
    "        print(f\"{label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "filtered_preds = []\n",
    "filtered_labels = []\n",
    "\n",
    "for pred_seq, label_seq in zip(preds, labels):\n",
    "    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n",
    "    filtered_label = [l for l in label_seq if l != -100]\n",
    "    filtered_preds.append(filtered_pred)\n",
    "    filtered_labels.append(filtered_label)\n",
    "\n",
    "print(\"\\n Running relaxed evaluation on test set (HPO_TERM only, no structure repair)...\")\n",
    "relaxed_metrics = relaxed_compute_metrics(filtered_preds, filtered_labels)\n",
    "print(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# -------------------\n",
    "# Constants & Paths\n",
    "# -------------------\n",
    "FILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\n",
    "DIR_SILVER  = Path(\"/kaggle/input/hpo-only\")\n",
    "OUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_BIO = OUT_DIR / \"train.jsonl\"\n",
    "DEV_BIO   = OUT_DIR / \"dev.jsonl\"\n",
    "TEST_BIO  = OUT_DIR / \"test.jsonl\"\n",
    "\n",
    "ENTITY_TYPES = {\n",
    "    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n",
    "    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# -------------------\n",
    "# Utility Functions\n",
    "# -------------------\n",
    "def iter_jsonl(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "def make_bio_labels(spans, enc):\n",
    "    tokens   = enc.tokens()\n",
    "    offsets  = enc[\"offset_mapping\"]\n",
    "    word_ids = enc.word_ids()\n",
    "    tags     = [\"O\"] * len(tokens)\n",
    "    span_to_tokens = []\n",
    "\n",
    "    for sp in spans:\n",
    "        s, e, typ = sp[\"start\"], sp[\"end\"], sp[\"label\"]\n",
    "        idxs = [\n",
    "            i for i, (b, t) in enumerate(offsets)\n",
    "            if not (t <= s or b >= e)\n",
    "        ]\n",
    "        span_to_tokens.append(idxs)\n",
    "        if not idxs:\n",
    "            continue\n",
    "        tags[idxs[0]] = f\"B-{typ}\"\n",
    "        for i in idxs[1:]:\n",
    "            tags[i] = f\"I-{typ}\"\n",
    "\n",
    "    for idxs in span_to_tokens:\n",
    "        if len(idxs) == 1:\n",
    "            tags[idxs[0]] = tags[idxs[0]].replace(\"I-\", \"B-\")\n",
    "\n",
    "    prev_wid = None\n",
    "    for i, wid in enumerate(word_ids):\n",
    "        if (wid is not None and wid == prev_wid\n",
    "                and tags[i] == \"O\"\n",
    "                and tags[i - 1].startswith((\"B-\", \"I-\"))):\n",
    "            tags[i] = \"I-\" + tags[i - 1][2:]\n",
    "        prev_wid = wid\n",
    "\n",
    "    return tags\n",
    "\n",
    "def record_to_bio(rec):\n",
    "    text  = rec.get(\"text\", \"\")\n",
    "    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n",
    "    if not spans:\n",
    "        return None\n",
    "    enc = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return {\n",
    "        \"tokens\": enc.tokens(),\n",
    "        \"labels\": make_bio_labels(spans, enc)\n",
    "    }\n",
    "\n",
    "def dump_jsonl(path: Path, data):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for obj in data:\n",
    "            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_extra_bio(path: Path):\n",
    "    extra = []\n",
    "    for rec in iter_jsonl(path):\n",
    "        bio = record_to_bio(rec)\n",
    "        if bio:\n",
    "            extra.append(bio)\n",
    "    return extra\n",
    "\n",
    "# -------------------\n",
    "# Step 1: Load and convert gold data\n",
    "# -------------------\n",
    "print(\">> Loading gold data …\")\n",
    "merged_bio = [\n",
    "    bio for rec in iter_jsonl(FILE_MERGED)\n",
    "    if (bio := record_to_bio(rec)) is not None\n",
    "]\n",
    "print(f\"Total valid records in gold: {len(merged_bio)}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 2: Random split gold data\n",
    "# -------------------\n",
    "\n",
    "train_dev, test_set = train_test_split(\n",
    "    merged_bio,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_set, dev_set = train_test_split(\n",
    "    train_dev,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 3: Add silver data to train set\n",
    "# -------------------\n",
    "extra_train = []\n",
    "if DIR_SILVER.exists():\n",
    "    print(\">> Loading silver data from hpo-only/\")\n",
    "    for jf in sorted(DIR_SILVER.glob(\"*.jsonl\")):\n",
    "        print(f\"  - {jf.name}\")\n",
    "        extra_train.extend(load_extra_bio(jf))\n",
    "else:\n",
    "    print(\">> Silver data directory not found.\")\n",
    "\n",
    "train_final = train_set + extra_train\n",
    "print(f\"Final train size: {len(train_final)} (including {len(extra_train)} silver records)\")\n",
    "\n",
    "# -------------------\n",
    "# Step 4: Save to disk\n",
    "# -------------------\n",
    "dump_jsonl(TRAIN_BIO, train_final)\n",
    "dump_jsonl(DEV_BIO, dev_set)\n",
    "dump_jsonl(TEST_BIO, test_set)\n",
    "\n",
    "print(f\"\\nSaved to:\")\n",
    "print(f\"  ➜ {TRAIN_BIO}\")\n",
    "print(f\"  ➜ {DEV_BIO}\")\n",
    "print(f\"  ➜ {TEST_BIO}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# 1. Load BIO datasets\n",
    "BIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "\n",
    "def keep_only_hpo_labels(example):\n",
    "    example[\"labels\"] = [\n",
    "        lab if lab.endswith(\"HPO_TERM\") else \"O\"\n",
    "        for lab in example[\"labels\"]\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "train_examples = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"train.jsonl\")]\n",
    "dev_examples   = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"dev.jsonl\")]\n",
    "test_examples  = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"test.jsonl\")]\n",
    "\n",
    "ds_splits = DatasetDict({\n",
    "    \"train\":      Dataset.from_list(train_examples),\n",
    "    \"validation\": Dataset.from_list(dev_examples),\n",
    "    \"test\":       Dataset.from_list(test_examples),\n",
    "})\n",
    "print(\"Loaded dataset sizes:\", {k: len(v) for k, v in ds_splits.items()})\n",
    "\n",
    "# 2. Tokenizer & label mapping\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "\n",
    "unique_labels = sorted({lab for ex in train_examples + dev_examples + test_examples\n",
    "                        for lab in ex[\"labels\"]})\n",
    "label2id = {lab: i for i, lab in enumerate(unique_labels)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "def tokenize_and_align_labels(ex):\n",
    "    enc = tokenizer(\n",
    "        ex[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    enc[\"labels\"] = [label2id[l] for l in ex[\"labels\"]]\n",
    "    return enc\n",
    "\n",
    "ds_splits = ds_splits.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=[\"tokens\", \"labels\"],\n",
    ")\n",
    "\n",
    "# 3. Model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# 4. Metrics\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    refs = p.label_ids\n",
    "    true_labels = [\n",
    "        [id2label[lid] for lid in seq if lid != -100] for seq in refs\n",
    "    ]\n",
    "    pred_labels = [\n",
    "        [id2label[pid] for pid, lid in zip(pred_seq, ref_seq) if lid != -100]\n",
    "        for pred_seq, ref_seq in zip(preds, refs)\n",
    "    ]\n",
    "    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "    return {\n",
    "        \"overall_precision\": result[\"overall_precision\"],\n",
    "        \"overall_recall\":    result[\"overall_recall\"],\n",
    "        \"overall_f1\":        result[\"overall_f1\"],\n",
    "        \"overall_accuracy\":  result[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# 5. Training arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner_pubmedbert\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[\"none\"],\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_splits[\"train\"],\n",
    "    eval_dataset=ds_splits[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 6. Train and evaluate\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# 7. Predict on test set\n",
    "test_metrics = trainer.predict(ds_splits[\"test\"]).metrics\n",
    "print(\"Test set metrics:\", test_metrics)\n",
    "predictions, labels, _ = trainer.predict(ds_splits[\"test\"])\n",
    "preds = predictions.argmax(-1)\n",
    "\n",
    "true_labels = [\n",
    "    [id2label[label_id] for label_id in seq if label_id != -100]\n",
    "    for seq in labels\n",
    "]\n",
    "pred_labels = [\n",
    "    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n",
    "    for pred_seq, label_seq in zip(preds, labels)\n",
    "]\n",
    "\n",
    "detailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "\n",
    "# 8. Only show HPO_TERM in per-label report\n",
    "print(\"\\n HPO_TERM classification report:\")\n",
    "for label, metrics in detailed_result.items():\n",
    "    if label.startswith(\"overall_\"):\n",
    "        continue\n",
    "    if label != \"HPO_TERM\":\n",
    "        continue\n",
    "    print(f\" {label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "\n",
    "predictions, labels, _ = trainer.predict(ds_splits[\"test\"])\n",
    "preds = predictions.argmax(-1)\n",
    "\n",
    "\n",
    "true_labels = [\n",
    "    [id2label[label_id] for label_id in seq if label_id != -100]\n",
    "    for seq in labels\n",
    "]\n",
    "pred_labels = [\n",
    "    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n",
    "    for pred_seq, label_seq in zip(preds, labels)\n",
    "]\n",
    "\n",
    "\n",
    "seqeval = load(\"seqeval\")\n",
    "detailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "\n",
    "\n",
    "print(f\"\\nOverall F1 score: {detailed_result.get('overall_f1', 0):.3f}\")\n",
    "\n",
    "\n",
    "print(\"\\n HPO_TERM classification report:\")\n",
    "hpo_metrics = detailed_result.get(\"HPO_TERM\")\n",
    "if hpo_metrics:\n",
    "    print(f\" {'HPO_TERM':20} | Precision: {hpo_metrics['precision']:.3f} | Recall: {hpo_metrics['recall']:.3f} | F1: {hpo_metrics['f1']:.3f}\")\n",
    "else:\n",
    "    print(\"No HPO_TERM entities found in predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_entities(labels):\n",
    "    spans = []\n",
    "    start = None\n",
    "    current_label = None\n",
    "    for i, lab_id in enumerate(labels):\n",
    "        label = id2label.get(lab_id, \"O\")\n",
    "        if label.startswith(\"B-HPO_TERM\"):\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "            start = i\n",
    "            current_label = \"HPO_TERM\"\n",
    "        elif label.startswith(\"I-HPO_TERM\") and current_label:\n",
    "            continue\n",
    "        else:\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "                current_label = None\n",
    "                start = None\n",
    "    if current_label:\n",
    "        spans.append((start, len(labels) - 1, current_label))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n",
    "    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n",
    "    return inter / union\n",
    "\n",
    "def relaxed_match(pred_span, true_span):\n",
    "    ps, pe, plabel = pred_span\n",
    "    ts, te, tlabel = true_span\n",
    "    if plabel != tlabel:\n",
    "        return False\n",
    "    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n",
    "        return True\n",
    "    if iou((ps, pe), (ts, te)) >= 0.4:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def relaxed_compute_metrics(preds, refs):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "\n",
    "    for pred_seq, ref_seq in zip(preds, refs):\n",
    "        pred_ents = extract_entities(pred_seq)\n",
    "        true_ents = extract_entities(ref_seq)\n",
    "        matched = set()\n",
    "\n",
    "        for pred_ent in pred_ents:\n",
    "            match_found = False\n",
    "            for i, true_ent in enumerate(true_ents):\n",
    "                if i in matched:\n",
    "                    continue\n",
    "                if relaxed_match(pred_ent, true_ent):\n",
    "                    tp += 1\n",
    "                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n",
    "                    matched.add(i)\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if not match_found:\n",
    "                fp += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n",
    "\n",
    "        for i, true_ent in enumerate(true_ents):\n",
    "            if i not in matched:\n",
    "                fn += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall    = tp / (tp + fn + 1e-10)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    print(\"\\n Relaxed Per-label HPO_TERM classification report:\")\n",
    "    for label, m in label_metrics.items():\n",
    "        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n",
    "        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n",
    "        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n",
    "        print(f\" {label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "filtered_preds = []\n",
    "filtered_labels = []\n",
    "\n",
    "for pred_seq, label_seq in zip(preds, labels):\n",
    "    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n",
    "    filtered_label = [l for l in label_seq if l != -100]\n",
    "    filtered_preds.append(filtered_pred)\n",
    "    filtered_labels.append(filtered_label)\n",
    "\n",
    "\n",
    "def clean_prediction_structure(labels):\n",
    "    cleaned = []\n",
    "    prev = \"O\"\n",
    "    for i, label in enumerate(labels):\n",
    "        if label.startswith(\"I-\") and prev == \"O\":\n",
    "            label = \"B-\" + label[2:]\n",
    "        if label == \"O\" and i+2 < len(labels) and labels[i+1].startswith(\"B-\") and labels[i+2].startswith(\"I-\"):\n",
    "            label = \"I-\" + labels[i+1][2:]\n",
    "        cleaned.append(label)\n",
    "        prev = label\n",
    "    return cleaned\n",
    "\n",
    "def fix_illegal_I(labels):\n",
    "    fixed = []\n",
    "    prev_type = \"O\"\n",
    "    for label in labels:\n",
    "        if label.startswith(\"I-\"):\n",
    "            if prev_type != label[2:]:\n",
    "                label = \"B-\" + label[2:]\n",
    "        fixed.append(label)\n",
    "        if label.startswith(\"B-\"):\n",
    "            prev_type = label[2:]\n",
    "        elif label.startswith(\"I-\"):\n",
    "            pass\n",
    "        else:\n",
    "            prev_type = \"O\"\n",
    "    return fixed\n",
    "\n",
    "def clean_and_fix_prediction_sequence(label_ids):\n",
    "\n",
    "    labels = [id2label.get(lid, \"O\") for lid in label_ids]\n",
    "    labels = clean_prediction_structure(labels)\n",
    "    labels = fix_illegal_I(labels)\n",
    "    return [label2id.get(l, 0) for l in labels]\n",
    "\n",
    "filtered_preds_cleaned = [clean_and_fix_prediction_sequence(seq) for seq in filtered_preds]\n",
    "\n",
    "print(\"\\n Running relaxed evaluation on test set (HPO_TERM only)...\")\n",
    "relaxed_metrics = relaxed_compute_metrics(filtered_preds_cleaned, filtered_labels)\n",
    "print(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def extract_entities(labels):\n",
    "    spans = []\n",
    "    start = None\n",
    "    current_label = None\n",
    "    for i, lab_id in enumerate(labels):\n",
    "        label = id2label.get(lab_id, \"O\")\n",
    "        if label.startswith(\"B-HPO_TERM\"):\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "            start = i\n",
    "            current_label = \"HPO_TERM\"\n",
    "        elif label.startswith(\"I-HPO_TERM\") and current_label:\n",
    "            continue\n",
    "        else:\n",
    "            if current_label:\n",
    "                spans.append((start, i - 1, current_label))\n",
    "                current_label = None\n",
    "                start = None\n",
    "    if current_label:\n",
    "        spans.append((start, len(labels) - 1, current_label))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n",
    "    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n",
    "    return inter / union\n",
    "\n",
    "def relaxed_match(pred_span, true_span):\n",
    "    ps, pe, plabel = pred_span\n",
    "    ts, te, tlabel = true_span\n",
    "    if plabel != tlabel:\n",
    "        return False\n",
    "    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n",
    "        return True\n",
    "    if iou((ps, pe), (ts, te)) >= 0.4:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def relaxed_compute_metrics(preds, refs):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n",
    "\n",
    "    for pred_seq, ref_seq in zip(preds, refs):\n",
    "        pred_ents = extract_entities(pred_seq)\n",
    "        true_ents = extract_entities(ref_seq)\n",
    "        matched = set()\n",
    "\n",
    "        for pred_ent in pred_ents:\n",
    "            match_found = False\n",
    "            for i, true_ent in enumerate(true_ents):\n",
    "                if i in matched:\n",
    "                    continue\n",
    "                if relaxed_match(pred_ent, true_ent):\n",
    "                    tp += 1\n",
    "                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n",
    "                    matched.add(i)\n",
    "                    match_found = True\n",
    "                    break\n",
    "            if not match_found:\n",
    "                fp += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n",
    "\n",
    "        for i, true_ent in enumerate(true_ents):\n",
    "            if i not in matched:\n",
    "                fn += 1\n",
    "                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-10)\n",
    "    recall    = tp / (tp + fn + 1e-10)\n",
    "    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "\n",
    "    print(\"\\nRelaxed Per-label HPO_TERM classification report:\")\n",
    "    for label, m in label_metrics.items():\n",
    "        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n",
    "        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n",
    "        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n",
    "        print(f\"{label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "filtered_preds = []\n",
    "filtered_labels = []\n",
    "\n",
    "for pred_seq, label_seq in zip(preds, labels):\n",
    "    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n",
    "    filtered_label = [l for l in label_seq if l != -100]\n",
    "    filtered_preds.append(filtered_pred)\n",
    "    filtered_labels.append(filtered_label)\n",
    "\n",
    "\n",
    "print(\"\\n Running relaxed evaluation on test set...\")\n",
    "relaxed_metrics = relaxed_compute_metrics(filtered_preds, filtered_labels)\n",
    "print(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"ner_pubmedbert_saved_HPO\")\n",
    "tokenizer.save_pretrained(\"ner_pubmedbert_saved_HPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install transformers obonet rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\n",
    "OUT_FILE  = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\n",
    "MAX_LENGTH = 512\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# label map: id -> label string, e.g. \"B-HPO_TERM\", \"I-HPO_TERM\", \"O\"\n",
    "id2label = model.config.id2label\n",
    "\n",
    "\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        text = syn.split('\"')[1]\n",
    "        hpo_map.setdefault(text.lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n",
    "    if matches:\n",
    "        return hpo_map[matches[0][0]][0]\n",
    "    return None\n",
    "\n",
    "\n",
    "normalized_mentions = []\n",
    "\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # Tokenize + truncate\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: encoding[k] for k in [\"input_ids\",\"attention_mask\"]})\n",
    "    logits = outputs.logits  # shape [1, seq_len, num_labels]\n",
    "    preds = logits.argmax(dim=-1)[0].cpu().tolist()  # [seq_len]\n",
    "    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()  # [(start,end),...]\n",
    "\n",
    "    # Extract contiguous HPO_TERM spans\n",
    "    span_start, span_end = None, None\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            # start new span\n",
    "            span_start = offsets[i][0]\n",
    "            span_end = offsets[i][1]\n",
    "        elif label == \"I-HPO_TERM\" and span_start is not None:\n",
    "            # continue span\n",
    "            span_end = offsets[i][1]\n",
    "        else:\n",
    "            # label is \"O\" or a new B- or outside; close existing span\n",
    "            if span_start is not None:\n",
    "                mention_text = sentence[span_start:span_end]\n",
    "                hpo_id = normalize_mention(mention_text)\n",
    "                normalized_mentions.append({\n",
    "                    \"sentence_index\": idx,\n",
    "                    \"sentence\":       sentence,\n",
    "                    \"mention\":        mention_text,\n",
    "                    \"span\":           (span_start, span_end),\n",
    "                    \"hpo_id\":         hpo_id\n",
    "                })\n",
    "                span_start, span_end = None, None\n",
    "            # no action on O or other B-\n",
    "\n",
    "    # if sentence ends with a span open\n",
    "    if span_start is not None:\n",
    "        mention_text = sentence[span_start:span_end]\n",
    "        hpo_id = normalize_mention(mention_text)\n",
    "        normalized_mentions.append({\n",
    "            \"sentence_index\": idx,\n",
    "            \"sentence\":       sentence,\n",
    "            \"mention\":        mention_text,\n",
    "            \"span\":           (span_start, span_end),\n",
    "            \"hpo_id\":         hpo_id\n",
    "        })\n",
    "\n",
    "\n",
    "total = len(normalized_mentions)\n",
    "mapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\n",
    "print(f\"Total mentions: {total}\")\n",
    "print(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\n",
    "print(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n",
    "\n",
    "\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for rec in normalized_mentions:\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Normalized results saved to: {OUT_FILE.resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\n",
    "OUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\n",
    "MAX_LENGTH = 512   \n",
    "DEVICE     = \"cuda:0\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "\n",
    "    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+%?\", \"\", text)\n",
    "\n",
    "    text = text.replace(\"##\", \"\")\n",
    "\n",
    "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def is_noise(mention: str) -> bool:\n",
    "\n",
    "    if re.fullmatch(r\"\\d+%?\", mention):\n",
    "        return True\n",
    "\n",
    "    if re.fullmatch(r\"[^\\w\\s]+\", mention):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_DIR, use_fast=True, local_files_only=True\n",
    ")\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_DIR, local_files_only=True\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "id2label = model.config.id2label\n",
    "\n",
    "\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph   = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        text = syn.split('\"')[1]\n",
    "        hpo_map.setdefault(text.lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n",
    "    return hpo_map[matches[0][0]][0] if matches else None\n",
    "\n",
    "\n",
    "normalized_mentions = []\n",
    "\n",
    "for idx, orig in enumerate(orig_sentences):\n",
    "\n",
    "    sentence = clean_text(orig)\n",
    "\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"]\n",
    "        )\n",
    "    logits  = outputs.logits[0]                  # [seq_len, num_labels]\n",
    "    preds   = logits.argmax(dim=-1).cpu().tolist()\n",
    "    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n",
    "\n",
    "\n",
    "    span_start = span_end = None\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            span_start, span_end = offsets[i]\n",
    "        elif label == \"I-HPO_TERM\" and span_start is not None:\n",
    "            span_end = offsets[i][1]\n",
    "        else:\n",
    "            if span_start is not None:\n",
    "                mention = sentence[span_start:span_end]\n",
    "  \n",
    "                if not is_noise(mention):\n",
    "                    normalized_mentions.append({\n",
    "                        \"sentence_index\": idx,\n",
    "                        \"sentence\":       sentence,\n",
    "                        \"mention\":        mention,\n",
    "                        \"span\":           (span_start, span_end),\n",
    "                        \"hpo_id\":         normalize_mention(mention)\n",
    "                    })\n",
    "                span_start = span_end = None\n",
    "\n",
    "\n",
    "    if span_start is not None:\n",
    "        mention = sentence[span_start:span_end]\n",
    "        if not is_noise(mention):\n",
    "            normalized_mentions.append({\n",
    "                \"sentence_index\": idx,\n",
    "                \"sentence\":       sentence,\n",
    "                \"mention\":        mention,\n",
    "                \"span\":           (span_start, span_end),\n",
    "                \"hpo_id\":         normalize_mention(mention)\n",
    "            })\n",
    "\n",
    "\n",
    "total  = len(normalized_mentions)\n",
    "mapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\n",
    "print(f\"Total mentions: {total}\")\n",
    "print(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\n",
    "print(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n",
    "\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for rec in normalized_mentions:\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Results saved to {OUT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\n",
    "OUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\n",
    "MAX_LENGTH = 512     \n",
    "DEVICE     = \"cuda:0\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"[UNK]\", \" \")\n",
    "\n",
    "    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\d+%?\", \"\", text)\n",
    "\n",
    "    text = text.replace(\"##\", \"\")\n",
    "\n",
    "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "def is_noise(mention: str) -> bool:\n",
    "\n",
    "    if re.fullmatch(r\"\\d+%?\", mention):\n",
    "        return True\n",
    "\n",
    "    if re.fullmatch(r\"[^\\w\\s]+\", mention):\n",
    "        return True\n",
    "\n",
    "    if len(mention.strip()) < 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_DIR, use_fast=True, local_files_only=True\n",
    ")\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_DIR, local_files_only=True\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "id2label = model.config.id2label\n",
    "\n",
    "\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph   = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        text = syn.split('\"')[1]\n",
    "        hpo_map.setdefault(text.lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n",
    "    return hpo_map[matches[0][0]][0] if matches else None\n",
    "\n",
    "\n",
    "normalized_mentions = []\n",
    "\n",
    "for idx, orig in enumerate(orig_sentences):\n",
    "\n",
    "    sentence = clean_text(orig)\n",
    "\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"]\n",
    "        )\n",
    "    logits  = outputs.logits[0]                  # [seq_len, num_labels]\n",
    "    preds   = logits.argmax(dim=-1).cpu().tolist()\n",
    "    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n",
    "\n",
    "\n",
    "    span_start = span_end = None\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            span_start, span_end = offsets[i]\n",
    "        elif label == \"I-HPO_TERM\" and span_start is not None:\n",
    "            span_end = offsets[i][1]\n",
    "        else:\n",
    "            if span_start is not None:\n",
    "                mention = sentence[span_start:span_end]\n",
    "\n",
    "                if not is_noise(mention):\n",
    "                    normalized_mentions.append({\n",
    "                        \"sentence_index\": idx,\n",
    "                        \"sentence\":       sentence,\n",
    "                        \"mention\":        mention,\n",
    "                        \"span\":           (span_start, span_end),\n",
    "                        \"hpo_id\":         normalize_mention(mention)\n",
    "                    })\n",
    "                span_start = span_end = None\n",
    "\n",
    "\n",
    "    if span_start is not None:\n",
    "        mention = sentence[span_start:span_end]\n",
    "        if not is_noise(mention):\n",
    "            normalized_mentions.append({\n",
    "                \"sentence_index\": idx,\n",
    "                \"sentence\":       sentence,\n",
    "                \"mention\":        mention,\n",
    "                \"span\":           (span_start, span_end),\n",
    "                \"hpo_id\":         normalize_mention(mention)\n",
    "            })\n",
    "\n",
    "\n",
    "total  = len(normalized_mentions)\n",
    "mapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\n",
    "print(f\"Total mentions: {total}\")\n",
    "print(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\n",
    "print(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n",
    "\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for rec in normalized_mentions:\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Results saved to {OUT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\n",
    "OUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\n",
    "MAX_LENGTH = 512\n",
    "DEVICE     = \"cuda:0\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"[UNK]\", \" \")\n",
    "    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+%?\", \"\", text)\n",
    "    text = text.replace(\"##\", \"\")\n",
    "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def is_noise(mention: str) -> bool:\n",
    "    if re.fullmatch(r\"\\d+%?\", mention): return True\n",
    "    if re.fullmatch(r\"[^\\w\\s]+\", mention): return True\n",
    "    if len(mention.strip()) < 3: return True\n",
    "    return False\n",
    "\n",
    "\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\n",
    "model.eval()\n",
    "id2label = model.config.id2label\n",
    "\n",
    "\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        text = syn.split('\"')[1]\n",
    "        hpo_map.setdefault(text.lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=85)\n",
    "    if matches:\n",
    "        return hpo_map[matches[0][0]][0]\n",
    "    return None\n",
    "\n",
    "\n",
    "normalized_mentions = []\n",
    "\n",
    "for idx, orig in enumerate(orig_sentences):\n",
    "    sentence = clean_text(orig)\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "\n",
    "\n",
    "    offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    logits = outputs.logits[0]  # shape: [seq_len, num_labels]\n",
    "    probs  = torch.softmax(logits, dim=-1)\n",
    "    preds  = probs.argmax(dim=-1).cpu().tolist()\n",
    "    scores = probs.max(dim=-1).values.cpu().tolist()\n",
    "    offsets = offset_mapping[0].tolist()\n",
    "    tokens  = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
    "\n",
    "    current_mention = \"\"\n",
    "    current_start = None\n",
    "    current_score = []\n",
    "\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        token = tokens[i]\n",
    "        offset = offsets[i]\n",
    "        score = scores[i]\n",
    "\n",
    "        if offset[0] == offset[1]:  # special tokens like [CLS], [SEP]\n",
    "            continue\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            if current_mention:\n",
    "                mention = current_mention.strip()\n",
    "                hpo_id = normalize_mention(mention)\n",
    "                avg_score = sum(current_score)/len(current_score) if current_score else 0\n",
    "                if hpo_id and not is_noise(mention) and avg_score > 0.6:\n",
    "                    normalized_mentions.append({\n",
    "                        \"sentence_index\": idx,\n",
    "                        \"sentence\":       sentence,\n",
    "                        \"mention\":        mention,\n",
    "                        \"span\":           (current_start, offset[0]),\n",
    "                        \"hpo_id\":         hpo_id\n",
    "                    })\n",
    "            current_mention = sentence[offset[0]:offset[1]]\n",
    "            current_start = offset[0]\n",
    "            current_score = [score]\n",
    "        elif label == \"I-HPO_TERM\" and current_mention:\n",
    "            current_mention += sentence[offset[0]:offset[1]]\n",
    "            current_score.append(score)\n",
    "        else:\n",
    "            if current_mention:\n",
    "                mention = current_mention.strip()\n",
    "                hpo_id = normalize_mention(mention)\n",
    "                avg_score = sum(current_score)/len(current_score) if current_score else 0\n",
    "                if hpo_id and not is_noise(mention) and avg_score > 0.6:\n",
    "                    normalized_mentions.append({\n",
    "                        \"sentence_index\": idx,\n",
    "                        \"sentence\":       sentence,\n",
    "                        \"mention\":        mention,\n",
    "                        \"span\":           (current_start, offset[0]),\n",
    "                        \"hpo_id\":         hpo_id\n",
    "                    })\n",
    "            current_mention = \"\"\n",
    "            current_score = []\n",
    "            current_start = None\n",
    "\n",
    "\n",
    "    if current_mention:\n",
    "        mention = current_mention.strip()\n",
    "        hpo_id = normalize_mention(mention)\n",
    "        avg_score = sum(current_score)/len(current_score) if current_score else 0\n",
    "        if hpo_id and not is_noise(mention) and avg_score > 0.6:\n",
    "            normalized_mentions.append({\n",
    "                \"sentence_index\": idx,\n",
    "                \"sentence\":       sentence,\n",
    "                \"mention\":        mention,\n",
    "                \"span\":           (current_start, len(sentence)),\n",
    "                \"hpo_id\":         hpo_id\n",
    "            })\n",
    "\n",
    "total  = len(normalized_mentions)\n",
    "mapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\n",
    "print(f\"Total mentions: {total}\")\n",
    "print(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\n",
    "print(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n",
    "\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for rec in normalized_mentions:\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Results saved to {OUT_FILE.resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install nltk obonet rapidfuzz transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE = Path(\"/kaggle/working/bio_outputs/test_text_only.jsonl\")\n",
    "MAX_LENGTH = 512\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"[UNK]\", \" \")\n",
    "    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+%?\", \"\", text)\n",
    "    text = text.replace(\"##\", \"\")\n",
    "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "def is_noise(mention: str) -> bool:\n",
    "    mention = mention.strip()\n",
    "    if not mention:\n",
    "        return True\n",
    "    if re.fullmatch(r\"\\d+%?\", mention):\n",
    "        return True\n",
    "    if re.fullmatch(r\"[^\\w\\s]+\", mention):\n",
    "        return True\n",
    "    if len(mention) < 3:\n",
    "        return True\n",
    "    if not re.search(r\"[aeiou]\", mention.lower()):\n",
    "        return True\n",
    "    if len(mention) > 25 and \" \" not in mention:\n",
    "        return True\n",
    "    blacklist = {\"showed\", \"found\", \"revealed\", \"including\", \"video\", \"fig\", \"fig.\", \"information\"}\n",
    "    if mention.lower() in blacklist:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# === Step 1: Load raw sentence text ===\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [ex[\"text\"] for ex in test_data]\n",
    "\n",
    "# === Step 2: Load model and tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\n",
    "model.eval()\n",
    "id2label = model.config.id2label\n",
    "\n",
    "# === Step 3: Build HPO dictionary from hp.obo ===\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        match = re.search(r'\"(.+?)\"', syn)\n",
    "        if match:\n",
    "            hpo_map.setdefault(match.group(1).lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    match = process.extractOne(key, hpo_map.keys(), score_cutoff=85)\n",
    "    if match:\n",
    "        return hpo_map[match[0]][0]\n",
    "    return None\n",
    "\n",
    "# === Step 4: Run NER + Normalize ===\n",
    "mapped_mentions = []\n",
    "unmapped_mentions = []\n",
    "\n",
    "for idx, orig in enumerate(orig_sentences):\n",
    "    sentence = clean_text(orig)\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    offset_mapping = encoding.pop(\"offset_mapping\")[0].tolist()\n",
    "    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    preds = outputs.logits.argmax(dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    current_offsets = []\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        start, end = offset_mapping[i]\n",
    "        if start == end:\n",
    "            continue\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            if current_offsets:\n",
    "                spans = current_offsets\n",
    "                mention = \" \".join([sentence[s:e] for s, e in spans]).strip()\n",
    "                if not is_noise(mention):\n",
    "                    hpo_id = normalize_mention(mention)\n",
    "                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n",
    "            current_offsets = [(start, end)]\n",
    "        elif label == \"I-HPO_TERM\" and current_offsets:\n",
    "            current_offsets.append((start, end))\n",
    "        else:\n",
    "            if current_offsets:\n",
    "                spans = current_offsets\n",
    "                mention = \" \".join([sentence[s:e] for s, e in spans]).strip()\n",
    "                if not is_noise(mention):\n",
    "                    hpo_id = normalize_mention(mention)\n",
    "                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n",
    "            current_offsets = []\n",
    "\n",
    "    # Last one\n",
    "    if current_offsets:\n",
    "        spans = current_offsets\n",
    "        mention = \" \".join([sentence[s:e] for s, e in spans]).strip()\n",
    "        if not is_noise(mention):\n",
    "            hpo_id = normalize_mention(mention)\n",
    "            (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n",
    "\n",
    "# === Step 5: Output\n",
    "print(f\"\\n Mapped Mentions ({len(mapped_mentions)}):\")\n",
    "for mention, hpo_id in mapped_mentions:\n",
    "    print(f\"{mention} --> {hpo_id}\")\n",
    "\n",
    "print(f\"\\nUnmapped Mentions ({len(unmapped_mentions)}):\")\n",
    "for mention, _ in unmapped_mentions:\n",
    "    print(mention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE = Path(\"/kaggle/working/bio_outputs/test_text_only.jsonl\")\n",
    "MAX_LENGTH = 512\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"[UNK]\", \" \")\n",
    "    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+%?\", \"\", text)\n",
    "    text = text.replace(\"##\", \"\")\n",
    "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def is_noise(mention: str) -> bool:\n",
    "    mention = mention.strip()\n",
    "    if not mention:\n",
    "        return True\n",
    "    if re.fullmatch(r\"\\d+%?\", mention): return True\n",
    "    if re.fullmatch(r\"[^\\w\\s]+\", mention): return True\n",
    "    if len(mention) < 3: return True\n",
    "    if not re.search(r\"[aeiou]\", mention.lower()): return True\n",
    "    if len(mention) > 25 and \" \" not in mention: return True\n",
    "    if len(mention.split()) < 2 and len(mention) <= 5: return True\n",
    "\n",
    "    blacklist = {\n",
    "        \"showed\", \"found\", \"revealed\", \"including\", \"video\", \"fig\", \"fig.\",\n",
    "        \"information\", \"inserted\", \"chinese\", \"data\", \"further\", \"proband\", \"thereafter\"\n",
    "    }\n",
    "    if mention.lower().strip(\".\") in blacklist:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# === Step 1: Load test data ===\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [ex[\"text\"] for ex in test_data]\n",
    "\n",
    "# === Step 2: Load model and tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\n",
    "model.eval()\n",
    "id2label = model.config.id2label\n",
    "\n",
    "# === Step 3: Load HPO terms from hp.obo ===\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        match = re.search(r'\"(.+?)\"', syn)\n",
    "        if match:\n",
    "            hpo_map.setdefault(match.group(1).lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    match = process.extractOne(key, hpo_map.keys(), score_cutoff=85)\n",
    "    if match:\n",
    "        return hpo_map[match[0]][0]\n",
    "    return None\n",
    "\n",
    "# === Step 4: Run NER + Normalize ===\n",
    "mapped_mentions = []\n",
    "unmapped_mentions = []\n",
    "\n",
    "for idx, orig in enumerate(orig_sentences):\n",
    "    sentence = clean_text(orig)\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        is_split_into_words=False\n",
    "    )\n",
    "    offset_mapping = encoding.pop(\"offset_mapping\")[0].tolist()\n",
    "    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    preds = outputs.logits.argmax(dim=-1)[0].cpu().tolist()\n",
    "\n",
    "    current_offsets = []\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        start, end = offset_mapping[i]\n",
    "        if start == end:\n",
    "            continue\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            if current_offsets:\n",
    "                spans = current_offsets\n",
    "                mention = \" \".join([sentence[s:e] for s, e in spans])\n",
    "                mention = re.sub(r\"^[^\\w]+\", \"\", mention)\n",
    "                mention = re.sub(r\"[^\\w]+$\", \"\", mention)\n",
    "                mention = re.sub(r\"\\s{2,}\", \" \", mention).strip()\n",
    "                if not is_noise(mention):\n",
    "                    hpo_id = normalize_mention(mention)\n",
    "                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n",
    "            current_offsets = [(start, end)]\n",
    "        elif label == \"I-HPO_TERM\" and current_offsets:\n",
    "            current_offsets.append((start, end))\n",
    "        else:\n",
    "            if current_offsets:\n",
    "                spans = current_offsets\n",
    "                mention = \" \".join([sentence[s:e] for s, e in spans])\n",
    "                mention = re.sub(r\"^[^\\w]+\", \"\", mention)\n",
    "                mention = re.sub(r\"[^\\w]+$\", \"\", mention)\n",
    "                mention = re.sub(r\"\\s{2,}\", \" \", mention).strip()\n",
    "                if not is_noise(mention):\n",
    "                    hpo_id = normalize_mention(mention)\n",
    "                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n",
    "            current_offsets = []\n",
    "\n",
    "\n",
    "    if current_offsets:\n",
    "        spans = current_offsets\n",
    "        mention = \" \".join([sentence[s:e] for s, e in spans])\n",
    "        mention = re.sub(r\"^[^\\w]+\", \"\", mention)\n",
    "        mention = re.sub(r\"[^\\w]+$\", \"\", mention)\n",
    "        mention = re.sub(r\"\\s{2,}\", \" \", mention).strip()\n",
    "        if not is_noise(mention):\n",
    "            hpo_id = normalize_mention(mention)\n",
    "            (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n",
    "\n",
    "\n",
    "print(f\"\\n Mapped Mentions ({len(mapped_mentions)}):\")\n",
    "for mention, hpo_id in mapped_mentions:\n",
    "    print(f\"{mention} --> {hpo_id}\")\n",
    "\n",
    "print(f\"\\n Unmapped Mentions ({len(unmapped_mentions)}):\")\n",
    "for mention, _ in unmapped_mentions:\n",
    "    print(mention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\n",
    "OUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\n",
    "MAX_LENGTH = 512\n",
    "DEVICE     = \"cuda:0\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace(\"[UNK]\", \" \")\n",
    "    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+%?\", \"\", text)\n",
    "    text = text.replace(\"##\", \"\")\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def is_noise(mention: str) -> bool:\n",
    "    mention = mention.strip().lower()\n",
    "    if not mention or len(mention) < 3:\n",
    "        return True\n",
    "    if mention in {\"showed\", \"had\", \"was\", \"were\", \"is\", \"are\", \"and\", \"or\", \"she\", \"he\"}:\n",
    "        return True\n",
    "    if mention.startswith(\",\") or mention.startswith(\".\") or mention.startswith(\" \"):\n",
    "        return True\n",
    "    if mention.count(\" \") >= 4:  \n",
    "        return True\n",
    "    if re.fullmatch(r\"[^\\w]+\", mention):\n",
    "        return True\n",
    "    if re.search(r\"\\b(?:he|she|it|they|this|that)\\b.*\\b(?:is|was|were|had|has|showed)\\b\", mention):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\n",
    "model.eval()\n",
    "id2label = model.config.id2label\n",
    "\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph   = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        syn_text = syn.split('\"')[1]\n",
    "        hpo_map.setdefault(syn_text.lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower().strip()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n",
    "    return hpo_map[matches[0][0]][0] if matches else None\n",
    "\n",
    "\n",
    "normalized_mentions = []\n",
    "\n",
    "for idx, raw in enumerate(orig_sentences):\n",
    "    sentence = clean_text(raw)\n",
    "    encoding = tokenizer(sentence, return_offsets_mapping=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: encoding[k] for k in [\"input_ids\", \"attention_mask\"]})\n",
    "    logits  = outputs.logits[0]\n",
    "    preds   = logits.argmax(dim=-1).cpu().tolist()\n",
    "    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n",
    "\n",
    "    span_start = span_end = None\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            span_start, span_end = offsets[i]\n",
    "        elif label == \"I-HPO_TERM\" and span_start is not None:\n",
    "            span_end = offsets[i][1]\n",
    "        else:\n",
    "            if span_start is not None:\n",
    "                mention = sentence[span_start:span_end]\n",
    "                mention_cleaned = mention.strip()\n",
    "                if not is_noise(mention_cleaned):\n",
    "                    normalized_mentions.append({\n",
    "                        \"sentence_index\": idx,\n",
    "                        \"sentence\":       sentence,\n",
    "                        \"mention\":        mention_cleaned,\n",
    "                        \"span\":           (span_start, span_end),\n",
    "                        \"hpo_id\":         normalize_mention(mention_cleaned)\n",
    "                    })\n",
    "                span_start = span_end = None\n",
    "\n",
    "    if span_start is not None:\n",
    "        mention = sentence[span_start:span_end]\n",
    "        mention_cleaned = mention.strip()\n",
    "        if not is_noise(mention_cleaned):\n",
    "            normalized_mentions.append({\n",
    "                \"sentence_index\": idx,\n",
    "                \"sentence\":       sentence,\n",
    "                \"mention\":        mention_cleaned,\n",
    "                \"span\":           (span_start, span_end),\n",
    "                \"hpo_id\":         normalize_mention(mention_cleaned)\n",
    "            })\n",
    "\n",
    "\n",
    "total  = len(normalized_mentions)\n",
    "mapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\n",
    "print(f\"Total mentions: {total}\")\n",
    "print(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\n",
    "print(f\"Failed to map:   {total - mapped} ({(total - mapped)/total:.1%})\")\n",
    "\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for rec in normalized_mentions:\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Results saved to {OUT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "import torch\n",
    "\n",
    "\n",
    "MODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\n",
    "OUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\n",
    "MAX_LENGTH = 512      \n",
    "DEVICE     = \"cuda:0\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_DIR, use_fast=True, local_files_only=True\n",
    ")\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_DIR, local_files_only=True\n",
    ").to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "id2label = model.config.id2label\n",
    "\n",
    "\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph   = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        text = syn.split('\"')[1]\n",
    "        hpo_map.setdefault(text.lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "\n",
    "    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=85)\n",
    "    if matches:\n",
    "        return hpo_map[matches[0][0]][0]\n",
    "    return None\n",
    "\n",
    "\n",
    "normalized_mentions = []\n",
    "\n",
    "for idx, orig in enumerate(orig_sentences):\n",
    "\n",
    "    sentence = clean_text(orig)\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        sentence,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"]\n",
    "        )\n",
    "    logits  = outputs.logits[0]  # [seq_len, num_labels]\n",
    "    preds   = logits.argmax(dim=-1).cpu().tolist()\n",
    "    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n",
    "\n",
    "\n",
    "    span_start = span_end = None\n",
    "    for i, label_id in enumerate(preds):\n",
    "        label = id2label[label_id]\n",
    "        if label == \"B-HPO_TERM\":\n",
    "            span_start, span_end = offsets[i]\n",
    "        elif label == \"I-HPO_TERM\" and span_start is not None:\n",
    "            span_end = offsets[i][1]\n",
    "        else:\n",
    "            if span_start is not None:\n",
    "                mention = sentence[span_start:span_end]\n",
    "                hpo_id  = normalize_mention(mention)\n",
    "                if hpo_id and not is_noise(mention):\n",
    "                    normalized_mentions.append({\n",
    "                        \"sentence_index\": idx,\n",
    "                        \"sentence\":       sentence,\n",
    "                        \"mention\":        mention,\n",
    "                        \"span\":           (span_start, span_end),\n",
    "                        \"hpo_id\":         hpo_id\n",
    "                    })\n",
    "                span_start = span_end = None\n",
    "\n",
    "    if span_start is not None:\n",
    "        mention = sentence[span_start:span_end]\n",
    "        hpo_id  = normalize_mention(mention)\n",
    "        if hpo_id and not is_noise(mention):\n",
    "            normalized_mentions.append({\n",
    "                \"sentence_index\": idx,\n",
    "                \"sentence\":       sentence,\n",
    "                \"mention\":        mention,\n",
    "                \"span\":           (span_start, span_end),\n",
    "                \"hpo_id\":         hpo_id\n",
    "            })\n",
    "\n",
    "total  = len(normalized_mentions)\n",
    "mapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\n",
    "print(f\"Total mentions: {total}\")\n",
    "print(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\n",
    "print(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n",
    "\n",
    "with OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "    for rec in normalized_mentions:\n",
    "        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Results saved to {OUT_FILE.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------\n",
    "# Constants & Paths\n",
    "# -------------------\n",
    "FILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\n",
    "DIR_SILVER  = Path(\"/kaggle/input/hpo-only\")\n",
    "OUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = OUT_DIR / \"train.jsonl\"\n",
    "DEV_FILE   = OUT_DIR / \"dev.jsonl\"\n",
    "TEST_FILE  = OUT_DIR / \"test.jsonl\"\n",
    "\n",
    "ENTITY_TYPES = {\n",
    "    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n",
    "    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n",
    "}\n",
    "\n",
    "# -------------------\n",
    "# Utility Functions\n",
    "# -------------------\n",
    "def iter_jsonl(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "def filter_valid_entities(rec):\n",
    "    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n",
    "    if spans:\n",
    "        return {\n",
    "            \"text\": rec[\"text\"],\n",
    "            \"spans\": spans\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def dump_jsonl(path: Path, data):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for obj in data:\n",
    "            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_filtered_silver(path: Path):\n",
    "    extra = []\n",
    "    for rec in iter_jsonl(path):\n",
    "        rec = filter_valid_entities(rec)\n",
    "        if rec:\n",
    "            extra.append(rec)\n",
    "    return extra\n",
    "\n",
    "# -------------------\n",
    "# Step 1: Load and convert gold data\n",
    "# -------------------\n",
    "print(\">> Loading gold data …\")\n",
    "merged_filtered = []\n",
    "for rec in iter_jsonl(FILE_MERGED):\n",
    "    filtered = filter_valid_entities(rec)\n",
    "    if filtered:\n",
    "        merged_filtered.append(filtered)\n",
    "print(f\"Total valid records in gold: {len(merged_filtered)}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 2: Split gold into train/dev/test\n",
    "# -------------------\n",
    "train_dev, test_set = train_test_split(\n",
    "    merged_filtered,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "train_set, dev_set = train_test_split(\n",
    "    train_dev,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 3: Add silver data to train set\n",
    "# -------------------\n",
    "extra_train = []\n",
    "if DIR_SILVER.exists():\n",
    "    print(\">> Loading silver data from hpo-only/\")\n",
    "    for jf in sorted(DIR_SILVER.glob(\"*.jsonl\")):\n",
    "        print(f\"  - {jf.name}\")\n",
    "        extra_train.extend(load_filtered_silver(jf))\n",
    "else:\n",
    "    print(\">> Silver data directory not found.\")\n",
    "\n",
    "train_final = train_set + extra_train\n",
    "print(f\"Final train size: {len(train_final)} (including {len(extra_train)} silver records)\")\n",
    "\n",
    "# -------------------\n",
    "# Step 4: Save to disk\n",
    "# -------------------\n",
    "dump_jsonl(TRAIN_FILE, train_final)\n",
    "dump_jsonl(DEV_FILE, dev_set)\n",
    "dump_jsonl(TEST_FILE, test_set)\n",
    "\n",
    "print(f\"\\nSaved to:\")\n",
    "print(f\"  ➜ {TRAIN_FILE}\")\n",
    "print(f\"  ➜ {DEV_FILE}\")\n",
    "print(f\"  ➜ {TEST_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# === 1. Load pre-split data with silver already included ===\n",
    "BIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "train_data = load_jsonl(BIO_DIR / \"train.jsonl\")\n",
    "dev_data   = load_jsonl(BIO_DIR / \"dev.jsonl\")\n",
    "test_data  = load_jsonl(BIO_DIR / \"test.jsonl\")\n",
    "\n",
    "ds_raw = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(dev_data),\n",
    "    \"test\": Dataset.from_list(test_data),\n",
    "})\n",
    "print(\"Loaded dataset sizes:\", {k: len(v) for k, v in ds_raw.items()})\n",
    "\n",
    "# === 2. Tokenizer and label mappings ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "label_list = [\"O\", \"B-HPO_TERM\", \"I-HPO_TERM\"]\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# === 3. Span-to-token label encoder ===\n",
    "def encode_and_align_labels(example):\n",
    "    text = example[\"text\"]\n",
    "    spans = example[\"spans\"]\n",
    "    entities = [(s[\"start\"], s[\"end\"]) for s in spans]\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for offset in encoding[\"offset_mapping\"]:\n",
    "        if offset == (0, 0):\n",
    "            labels.append(\"O\")\n",
    "            continue\n",
    "        tag = \"O\"\n",
    "        for start, end in entities:\n",
    "            if offset[0] >= start and offset[1] <= end:\n",
    "                tag = \"B-HPO_TERM\" if offset[0] == start else \"I-HPO_TERM\"\n",
    "                break\n",
    "        labels.append(tag)\n",
    "\n",
    "    encoding[\"labels\"] = [label2id[l] for l in labels]\n",
    "    return encoding\n",
    "\n",
    "# === 4. Encode all splits ===\n",
    "ds_encoded = ds_raw.map(\n",
    "    encode_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=[\"text\", \"spans\"]\n",
    ")\n",
    "print(\"Encoding complete.\")\n",
    "\n",
    "# === 5. Load model ===\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# === 6. Evaluation metrics ===\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    true_labels = [\n",
    "        [id2label[lid] for lid in seq if lid != -100]\n",
    "        for seq in labels\n",
    "    ]\n",
    "    pred_labels = [\n",
    "        [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n",
    "        for pred_seq, label_seq in zip(preds, labels)\n",
    "    ]\n",
    "    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "    return {\n",
    "        \"overall_precision\": result[\"overall_precision\"],\n",
    "        \"overall_recall\":    result[\"overall_recall\"],\n",
    "        \"overall_f1\":        result[\"overall_f1\"],\n",
    "        \"overall_accuracy\":  result[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# === 7. Training configuration ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner_pubmedbert\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[\"none\"],\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_encoded[\"train\"],\n",
    "    eval_dataset=ds_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# === 8. Train and evaluate ===\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# === 9. Predict on test set ===\n",
    "print(\"\\n--- Predicting on test set ---\")\n",
    "pred_output = trainer.predict(ds_encoded[\"test\"])\n",
    "preds = pred_output.predictions.argmax(-1)\n",
    "labels = pred_output.label_ids\n",
    "\n",
    "true_labels = [\n",
    "    [id2label[lid] for lid in seq if lid != -100]\n",
    "    for seq in labels\n",
    "]\n",
    "pred_labels = [\n",
    "    [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n",
    "    for pred_seq, label_seq in zip(preds, labels)\n",
    "]\n",
    "\n",
    "detailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "\n",
    "print(\"\\nHPO_TERM classification report:\")\n",
    "for label, metrics in detailed_result.items():\n",
    "    if label == \"HPO_TERM\":\n",
    "        print(f\"{label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE = Path(\"/kaggle/working/bio_outputs/test.jsonl\") \n",
    "MAX_LENGTH = 512\n",
    "DEVICE = 0  # use -1 for CPU (if no GPU)\n",
    "\n",
    "# === Step 1: Load test data ===\n",
    "print(\">> Loading test data\")\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [ex[\"text\"] for ex in test_data]\n",
    "\n",
    "# === Step 2: Load model and tokenizer with pipeline ===\n",
    "print(\">> Loading model and tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\", \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# === Step 3: Run NER without post-processing\n",
    "print(\">> Running NER without post-processing\")\n",
    "all_results = []\n",
    "\n",
    "for idx, sentence in enumerate(orig_sentences):\n",
    "    results = ner_pipeline(sentence)\n",
    "    for ent in results:\n",
    "        word = ent[\"word\"]\n",
    "        start = ent[\"start\"]\n",
    "        end = ent[\"end\"]\n",
    "        label = ent[\"entity_group\"]\n",
    "        score = ent[\"score\"]\n",
    "        all_results.append({\n",
    "            \"sentence_idx\": idx,\n",
    "            \"text\": sentence,\n",
    "            \"mention\": word,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"label\": label,\n",
    "            \"score\": round(score, 4)\n",
    "        })\n",
    "\n",
    "# === Step 4: Print Results\n",
    "print(f\"\\nTotal mentions extracted: {len(all_results)}\")\n",
    "for r in all_results:\n",
    "    print(f\"[{r['label']}] {r['mention']} (score={r['score']}, span={r['start']}-{r['end']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import obonet\n",
    "from rapidfuzz import process\n",
    "\n",
    "# === Config ===\n",
    "MODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\n",
    "TEST_FILE = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\n",
    "MAX_LENGTH = 512\n",
    "DEVICE = 0  # use -1 for CPU\n",
    "\n",
    "# === Step 1: Load test data ===\n",
    "print(\">> Loading test data\")\n",
    "test_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\n",
    "orig_sentences = [ex[\"text\"] for ex in test_data]\n",
    "\n",
    "# === Step 2: Load model and tokenizer with pipeline ===\n",
    "print(\">> Loading model and tokenizer\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# === Step 3: Load HPO terms from hp.obo\n",
    "print(\">> Loading HPO terms from obo\")\n",
    "obo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\n",
    "graph = obonet.read_obo(obo_url)\n",
    "hpo_map = {}\n",
    "for node_id, data in graph.nodes(data=True):\n",
    "    name = data.get(\"name\")\n",
    "    if name:\n",
    "        hpo_map.setdefault(name.lower(), []).append(node_id)\n",
    "    for syn in data.get(\"synonym\", []):\n",
    "        match = re.search(r'\"(.+?)\"', syn)\n",
    "        if match:\n",
    "            hpo_map.setdefault(match.group(1).lower(), []).append(node_id)\n",
    "\n",
    "def normalize_mention(text: str):\n",
    "    key = text.lower()\n",
    "    if key in hpo_map:\n",
    "        return hpo_map[key][0]\n",
    "    match = process.extractOne(key, hpo_map.keys(), score_cutoff=85)\n",
    "    if match:\n",
    "        return hpo_map[match[0]][0]\n",
    "    return None\n",
    "\n",
    "# === Step 4: Run NER + Normalize (No Noise Filtering)\n",
    "print(\">> Running NER and normalization (no filtering)\")\n",
    "mapped_mentions = []\n",
    "unmapped_mentions = []\n",
    "\n",
    "for idx, sentence in enumerate(orig_sentences):\n",
    "    results = ner_pipeline(sentence)\n",
    "    for ent in results:\n",
    "        mention = ent[\"word\"].strip()\n",
    "        hpo_id = normalize_mention(mention)\n",
    "        (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n",
    "\n",
    "# === Step 5: Output\n",
    "print(f\"\\nMapped Mentions ({len(mapped_mentions)}):\")\n",
    "for mention, hpo_id in mapped_mentions:\n",
    "    print(f\"{mention} --> {hpo_id}\")\n",
    "\n",
    "print(f\"\\nUnmapped Mentions ({len(unmapped_mentions)}):\")\n",
    "for mention, _ in unmapped_mentions:\n",
    "    print(mention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------\n",
    "# Constants & Paths\n",
    "# -------------------\n",
    "FILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\n",
    "SILVER_FILE = Path(\"/kaggle/input/hpo-only/HPO_only.jsonl\")\n",
    "OUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_FILE = OUT_DIR / \"train.jsonl\"\n",
    "DEV_FILE   = OUT_DIR / \"dev.jsonl\"\n",
    "TEST_FILE  = OUT_DIR / \"test.jsonl\"\n",
    "\n",
    "ENTITY_TYPES = {\n",
    "    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n",
    "    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n",
    "}\n",
    "\n",
    "# -------------------\n",
    "# Utility Functions\n",
    "# -------------------\n",
    "def iter_jsonl(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "def filter_valid_entities(rec):\n",
    "    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n",
    "    if spans:\n",
    "        return {\n",
    "            \"text\": rec[\"text\"],\n",
    "            \"spans\": spans\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def dump_jsonl(path: Path, data):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for obj in data:\n",
    "            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# -------------------\n",
    "# Step 1: Load and convert gold data\n",
    "# -------------------\n",
    "print(\">> Loading gold data …\")\n",
    "merged_filtered = []\n",
    "for rec in iter_jsonl(FILE_MERGED):\n",
    "    filtered = filter_valid_entities(rec)\n",
    "    if filtered:\n",
    "        merged_filtered.append(filtered)\n",
    "print(f\"Total valid records in gold: {len(merged_filtered)}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 2: Split gold into train/dev/test\n",
    "# -------------------\n",
    "train_dev, test_set = train_test_split(\n",
    "    merged_filtered,\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "train_set, dev_set = train_test_split(\n",
    "    train_dev,\n",
    "    test_size=0.25,\n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n",
    "\n",
    "# -------------------\n",
    "# Step 3: Add silver data to train set\n",
    "# -------------------\n",
    "extra_train = []\n",
    "if SILVER_FILE.exists():\n",
    "    print(\">> Loading silver data from HPO_only.jsonl\")\n",
    "    for rec in iter_jsonl(SILVER_FILE):\n",
    "        filtered = filter_valid_entities(rec)\n",
    "        if filtered:\n",
    "            extra_train.append(filtered)\n",
    "    print(f\"  ➜ Loaded {len(extra_train)} silver records.\")\n",
    "else:\n",
    "    print(f\">> Silver file not found: {SILVER_FILE}\")\n",
    "\n",
    "train_final = train_set + extra_train\n",
    "print(f\"Final train size: {len(train_final)} (including {len(extra_train)} silver records)\")\n",
    "\n",
    "# -------------------\n",
    "# Step 4: Save to disk\n",
    "# -------------------\n",
    "dump_jsonl(TRAIN_FILE, train_final)\n",
    "dump_jsonl(DEV_FILE, dev_set)\n",
    "dump_jsonl(TEST_FILE, test_set)\n",
    "\n",
    "print(f\"\\nSaved to:\")\n",
    "print(f\"  ➜ {TRAIN_FILE}\")\n",
    "print(f\"  ➜ {DEV_FILE}\")\n",
    "print(f\"  ➜ {TEST_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# === 1. Load pre-split data with silver already included ===\n",
    "BIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "train_data = load_jsonl(BIO_DIR / \"train.jsonl\")\n",
    "dev_data   = load_jsonl(BIO_DIR / \"dev.jsonl\")\n",
    "test_data  = load_jsonl(BIO_DIR / \"test.jsonl\")\n",
    "\n",
    "ds_raw = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(dev_data),\n",
    "    \"test\": Dataset.from_list(test_data),\n",
    "})\n",
    "print(\" Loaded dataset sizes:\", {k: len(v) for k, v in ds_raw.items()})\n",
    "\n",
    "# === 2. Tokenizer and label mappings ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "label_list = [\"O\", \"B-HPO_TERM\", \"I-HPO_TERM\"]\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# === 3. Span-to-token label encoder ===\n",
    "def encode_and_align_labels(example):\n",
    "    text = example[\"text\"]\n",
    "    spans = example[\"spans\"]\n",
    "    entities = [(s[\"start\"], s[\"end\"]) for s in spans]\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for offset in encoding[\"offset_mapping\"]:\n",
    "        if offset == (0, 0):\n",
    "            labels.append(\"O\")\n",
    "            continue\n",
    "        tag = \"O\"\n",
    "        for start, end in entities:\n",
    "            if offset[0] >= start and offset[1] <= end:\n",
    "                tag = \"B-HPO_TERM\" if offset[0] == start else \"I-HPO_TERM\"\n",
    "                break\n",
    "        labels.append(tag)\n",
    "\n",
    "    encoding[\"labels\"] = [label2id[l] for l in labels]\n",
    "    return encoding\n",
    "\n",
    "# === 4. Encode all splits ===\n",
    "ds_encoded = ds_raw.map(\n",
    "    encode_and_align_labels,\n",
    "    batched=False,\n",
    "    remove_columns=[\"text\", \"spans\"]\n",
    ")\n",
    "print(\"Encoding complete.\")\n",
    "\n",
    "# === 5. Load model ===\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# === 6. Evaluation metrics ===\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    true_labels = [\n",
    "        [id2label[lid] for lid in seq if lid != -100]\n",
    "        for seq in labels\n",
    "    ]\n",
    "    pred_labels = [\n",
    "        [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n",
    "        for pred_seq, label_seq in zip(preds, labels)\n",
    "    ]\n",
    "    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "    return {\n",
    "        \"overall_precision\": result[\"overall_precision\"],\n",
    "        \"overall_recall\":    result[\"overall_recall\"],\n",
    "        \"overall_f1\":        result[\"overall_f1\"],\n",
    "        \"overall_accuracy\":  result[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# === 7. Training configuration ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"ner_pubmedbert\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"overall_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[\"none\"],\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_encoded[\"train\"],\n",
    "    eval_dataset=ds_encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# === 8. Train and evaluate ===\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "# === 9. Predict on test set ===\n",
    "print(\"\\n--- Predicting on test set ---\")\n",
    "pred_output = trainer.predict(ds_encoded[\"test\"])\n",
    "preds = pred_output.predictions.argmax(-1)\n",
    "labels = pred_output.label_ids\n",
    "\n",
    "true_labels = [\n",
    "    [id2label[lid] for lid in seq if lid != -100]\n",
    "    for seq in labels\n",
    "]\n",
    "pred_labels = [\n",
    "    [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n",
    "    for pred_seq, label_seq in zip(preds, labels)\n",
    "]\n",
    "\n",
    "detailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n",
    "\n",
    "print(\"\\nHPO_TERM classification report:\")\n",
    "for label, metrics in detailed_result.items():\n",
    "    if label == \"HPO_TERM\":\n",
    "        print(f\"{label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13164677,
     "datasetId": 7727360,
     "sourceId": 12572436,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13317396,
     "datasetId": 8022610,
     "sourceId": 12705014,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 13236735,
     "datasetId": 7983789,
     "sourceId": 12634678,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
