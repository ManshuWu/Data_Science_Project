{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":12572436,"datasetId":7727360,"databundleVersionId":13164677},{"sourceType":"datasetVersion","sourceId":12705014,"datasetId":8022610,"databundleVersionId":13317396},{"sourceType":"datasetVersion","sourceId":12634678,"datasetId":7983789,"databundleVersionId":13236735}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nINPUT_DIR = Path(\"/kaggle/input/fullfull/annotations\")\nOUTPUT_FILE = Path(\"merged_spans_with_entities.jsonl\")\n\nmerged = []\n\nfor span_path in sorted(INPUT_DIR.glob(\"*_spans.jsonl\")):\n    filename = span_path.name\n    with span_path.open(\"r\", encoding=\"utf-8\") as f:\n        for lineno, line in enumerate(f, start=1):\n            line = line.strip()\n            if not line:\n                print(f\"Skipping empty line at {filename}:{lineno}\")\n                continue\n            try:\n                rec = json.loads(line)\n            except json.JSONDecodeError as e:\n                print(f\"JSON decode error at {filename}:{lineno} — {e}\")\n                continue\n\n            spans = rec.get(\"spans\", [])\n            if not spans:\n                continue\n\n            entry = {\n                \"text\": rec.get(\"text\", \"\"),\n                \"tokens\": rec.get(\"tokens\", []),\n                \"spans\": spans,\n            }\n            merged.append(entry)\n\nwith OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as fw:\n    for entry in merged:\n        fw.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Merged and saved {len(merged)} entity-containing records to: {OUTPUT_FILE.resolve()}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nINPUT_DIR = Path(\"/kaggle/input/fullfull/annotations\")      # 修改为你的输入目录\nOUTPUT_FILE = Path(\"merged_spans.jsonl\")                    # 输出文件名可按需调整\n\nmerged = []\n\nfor span_path in sorted(INPUT_DIR.glob(\"*_spans.jsonl\")):\n    filename = span_path.name\n    with span_path.open(\"r\", encoding=\"utf-8\") as f:\n        for lineno, line in enumerate(f, start=1):\n            line = line.strip()\n            if not line:\n                print(f\"Skipping empty line at {filename}:{lineno}\")\n                continue\n            try:\n                rec = json.loads(line)\n            except json.JSONDecodeError as e:\n                print(f\"JSON decode error at {filename}:{lineno} — {e}\")\n                continue\n\n            spans = rec.get(\"spans\", [])\n            if not spans:\n                continue\n\n            # 只保留 text 和 spans\n            merged.append({\n                \"text\": rec.get(\"text\", \"\"),\n                \"spans\": spans,\n            })\n\nwith OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as fw:\n    for entry in merged:\n        fw.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Merged and saved {len(merged)} records (text + spans) to: {OUTPUT_FILE.resolve()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nimport pandas as pd\n\n# --- CONFIGURATION ---\nINPUT_XLSX = Path(\"/kaggle/input/polgtable/subset_POLG.xlsx\")\nOUTPUT_FILE = Path(\"hpo_terms.jsonl\")\n\n# --- STEP 1: 读取数据 ---\ndf = pd.read_excel(INPUT_XLSX, engine=\"openpyxl\")\n\n# --- STEP 2: 提取所有 HPO 术语（保留顺序 + 去重）---\nall_hpo_terms = []\nseen = set()\n\nfor terms in df[\"HPO_Term\"]:\n    if pd.isna(terms):\n        continue\n    for term in str(terms).split(\";\"):\n        term = term.strip()\n        if term and term not in seen:\n            all_hpo_terms.append(term)\n            seen.add(term)\n\n# --- STEP 3: 写入 JSONL 文件 ---\nwith OUTPUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n    for hpo in all_hpo_terms:\n        fout.write(json.dumps({\"HPO_TERM\": hpo}, ensure_ascii=False) + \"\\n\")\n\nprint(f\"✅ 提取完成，共写入 {len(all_hpo_terms)} 个 HPO_TERM 到 {OUTPUT_FILE.resolve()}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom collections import Counter, defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\n\n# -------------------\n# Constants & Paths\n# -------------------\nFILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\nOUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_BIO = OUT_DIR / \"train.jsonl\"\nDEV_BIO   = OUT_DIR / \"dev.jsonl\"\nTEST_BIO  = OUT_DIR / \"test.jsonl\"\nTEST_TEXT_ONLY = OUT_DIR / \"test_text_only.jsonl\"   # ✅ 新增路径\n\nENTITY_TYPES = {\n    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n}\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    use_fast=True\n)\n\n# -------------------\n# Utility Functions\n# -------------------\ndef iter_jsonl(path: Path):\n    with path.open(\"r\", encoding=\"utf-8\") as fh:\n        for line in fh:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                yield json.loads(line)\n            except json.JSONDecodeError:\n                continue\n\ndef make_bio_labels(spans, enc):\n    tokens   = enc.tokens()\n    offsets  = enc[\"offset_mapping\"]\n    word_ids = enc.word_ids()\n    tags     = [\"O\"] * len(tokens)\n    span_to_tokens = []\n\n    for sp in spans:\n        s, e, typ = sp[\"start\"], sp[\"end\"], sp[\"label\"]\n        idxs = [\n            i for i, (b, t) in enumerate(offsets)\n            if not (t <= s or b >= e)\n        ]\n        span_to_tokens.append(idxs)\n        if not idxs:\n            continue\n        tags[idxs[0]] = f\"B-{typ}\"\n        for i in idxs[1:]:\n            tags[i] = f\"I-{typ}\"\n\n    for idxs in span_to_tokens:\n        if len(idxs) == 1:\n            tags[idxs[0]] = tags[idxs[0]].replace(\"I-\", \"B-\")\n\n    prev_wid = None\n    for i, wid in enumerate(word_ids):\n        if (wid is not None and wid == prev_wid\n                and tags[i] == \"O\"\n                and tags[i - 1].startswith((\"B-\", \"I-\"))):\n            tags[i] = \"I-\" + tags[i - 1][2:]\n        prev_wid = wid\n\n    return tags\n\ndef record_to_bio(rec):\n    text  = rec.get(\"text\", \"\")\n    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n    if not spans:\n        return None\n    enc = tokenizer(\n        text,\n        add_special_tokens=False,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=512\n    )\n    return {\n        \"text\": text,  # ✅ 保留原始句子，供后续保存\n        \"tokens\": enc.tokens(),\n        \"labels\": make_bio_labels(spans, enc)\n    }\n\ndef dump_jsonl(path: Path, data):\n    with path.open(\"w\", encoding=\"utf-8\") as fh:\n        for obj in data:\n            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n\n# -------------------\n# Load & Convert\n# -------------------\nprint(\">> Loading and converting gold-standard data …\")\nmerged_bio = [\n    bio for rec in iter_jsonl(FILE_MERGED)\n    if (bio := record_to_bio(rec)) is not None\n]\nprint(f\"Total valid records: {len(merged_bio)}\")\n\n# -------------------\n# Simple Random Split (70/20/10)\n# -------------------\ntrain_set, temp_set = train_test_split(\n    merged_bio, test_size=0.3, random_state=42\n)\ndev_set, test_set = train_test_split(\n    temp_set, test_size=1/3, random_state=42\n)\n\nprint(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n\n# -------------------\n# Save BIO Format Files\n# -------------------\ndump_jsonl(TRAIN_BIO, train_set)\ndump_jsonl(DEV_BIO, dev_set)\ndump_jsonl(TEST_BIO, test_set)\n\n# -------------------\n# ✅ Save only raw test set texts\n# -------------------\nwith TEST_TEXT_ONLY.open(\"w\", encoding=\"utf-8\") as fw:\n    for ex in test_set:\n        if \"text\" in ex:\n            fw.write(json.dumps({\"text\": ex[\"text\"]}, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Saved to {TRAIN_BIO}, {DEV_BIO}, {TEST_BIO}\")\nprint(f\"✅ Raw test text saved to: {TEST_TEXT_ONLY}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install seqeval evaluate torchcrf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    DataCollatorForTokenClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport evaluate\n\n# 1. Load BIO datasets\nBIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n\ndef load_jsonl(path: Path):\n    with path.open(encoding=\"utf-8\") as f:\n        return [json.loads(line) for line in f if line.strip()]\n\n# 1.1 过滤非 HPO_TERM 标签为 'O'\ndef keep_only_hpo_labels(example):\n    example[\"labels\"] = [\n        lab if lab.endswith(\"HPO_TERM\") else \"O\"\n        for lab in example[\"labels\"]\n    ]\n    return example\n\ntrain_examples = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"train.jsonl\")]\ndev_examples   = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"dev.jsonl\")]\ntest_examples  = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"test.jsonl\")]\n\nds_splits = DatasetDict({\n    \"train\":      Dataset.from_list(train_examples),\n    \"validation\": Dataset.from_list(dev_examples),\n    \"test\":       Dataset.from_list(test_examples),\n})\nprint(\"Loaded dataset sizes:\", {k: len(v) for k, v in ds_splits.items()})\n\n# 2. Tokenizer & label mapping\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    use_fast=True,\n)\n\n# 2.1 只保留 ['B-HPO_TERM', 'I-HPO_TERM', 'O']\nunique_labels = sorted({lab for ex in train_examples + dev_examples + test_examples\n                        for lab in ex[\"labels\"]})\nlabel2id = {lab: i for i, lab in enumerate(unique_labels)}\nid2label = {i: lab for lab, i in label2id.items()}\n\ndef tokenize_and_align_labels(ex):\n    enc = tokenizer(\n        ex[\"tokens\"],\n        is_split_into_words=True,\n        truncation=True,\n        max_length=512,\n        return_attention_mask=True,\n    )\n    enc[\"labels\"] = [label2id[l] for l in ex[\"labels\"]]\n    return enc\n\nds_splits = ds_splits.map(\n    tokenize_and_align_labels,\n    batched=False,\n    remove_columns=[\"tokens\", \"labels\"],\n)\n\n# 3. Model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    num_labels=len(unique_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\n\n# 4. Metrics\nseqeval = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    refs = p.label_ids\n    true_labels = [\n        [id2label[lid] for lid in seq if lid != -100] for seq in refs\n    ]\n    pred_labels = [\n        [id2label[pid] for pid, lid in zip(pred_seq, ref_seq) if lid != -100]\n        for pred_seq, ref_seq in zip(preds, refs)\n    ]\n    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n    return {\n        \"overall_precision\": result[\"overall_precision\"],\n        \"overall_recall\":    result[\"overall_recall\"],\n        \"overall_f1\":        result[\"overall_f1\"],\n        \"overall_accuracy\":  result[\"overall_accuracy\"],\n    }\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\n# 5. Training arguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"ner_pubmedbert\",\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"overall_f1\",\n    greater_is_better=True,\n    report_to=[\"none\"],\n    save_total_limit=1,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_splits[\"train\"],\n    eval_dataset=ds_splits[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# 6. Train and evaluate\ntrainer.train()\ntrainer.evaluate()\n\n# 7. Predict on test set\ntest_metrics = trainer.predict(ds_splits[\"test\"]).metrics\nprint(\"Test set metrics:\", test_metrics)\npredictions, labels, _ = trainer.predict(ds_splits[\"test\"])\npreds = predictions.argmax(-1)\n\ntrue_labels = [\n    [id2label[label_id] for label_id in seq if label_id != -100]\n    for seq in labels\n]\npred_labels = [\n    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n    for pred_seq, label_seq in zip(preds, labels)\n]\n\ndetailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n\n# 8. Only show HPO_TERM in per-label report\nprint(\"\\n HPO_TERM classification report:\")\nfor label, metrics in detailed_result.items():\n    if label.startswith(\"overall_\"):\n        continue\n    if label != \"HPO_TERM\":\n        continue\n    print(f\" {label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from evaluate import load\n\n# 预测\npredictions, labels, _ = trainer.predict(ds_splits[\"test\"])\npreds = predictions.argmax(-1)\n\n# 还原标签ID为标签名（跳过 -100，即 ignore_index）\ntrue_labels = [\n    [id2label[label_id] for label_id in seq if label_id != -100]\n    for seq in labels\n]\npred_labels = [\n    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n    for pred_seq, label_seq in zip(preds, labels)\n]\n\n# 使用 seqeval 计算所有标签评估结果\nseqeval = load(\"seqeval\")\ndetailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n\n# 打印 overall F1（可选）\nprint(f\"\\nOverall F1 score: {detailed_result.get('overall_f1', 0):.3f}\")\n\n# 只输出 HPO_TERM 的结果\nprint(\"\\n HPO_TERM classification report:\")\nhpo_metrics = detailed_result.get(\"HPO_TERM\")\nif hpo_metrics:\n    print(f\" {'HPO_TERM':20} | Precision: {hpo_metrics['precision']:.3f} | Recall: {hpo_metrics['recall']:.3f} | F1: {hpo_metrics['f1']:.3f}\")\nelse:\n    print(\"No HPO_TERM entities found in predictions.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# ----------- Step 1: 提取 HPO_TERM 实体 spans -----------\ndef extract_entities(labels):\n    spans = []\n    start = None\n    current_label = None\n    for i, lab_id in enumerate(labels):\n        label = id2label.get(lab_id, \"O\")\n        if label.startswith(\"B-HPO_TERM\"):\n            if current_label:\n                spans.append((start, i - 1, current_label))\n            start = i\n            current_label = \"HPO_TERM\"\n        elif label.startswith(\"I-HPO_TERM\") and current_label:\n            continue\n        else:\n            if current_label:\n                spans.append((start, i - 1, current_label))\n                current_label = None\n                start = None\n    if current_label:\n        spans.append((start, len(labels) - 1, current_label))\n    return spans\n\n# ----------- Step 2: IOU计算 & Relaxed匹配 -----------\ndef iou(a, b):\n    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n    return inter / union\n\ndef relaxed_match(pred_span, true_span):\n    ps, pe, plabel = pred_span\n    ts, te, tlabel = true_span\n    if plabel != tlabel:\n        return False\n    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n        return True\n    if iou((ps, pe), (ts, te)) >= 0.4:\n        return True\n    return False\n\n# ----------- Step 3: Relaxed Evaluation Metric ----------\ndef relaxed_compute_metrics(preds, refs):\n    tp, fp, fn = 0, 0, 0\n    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n\n    for pred_seq, ref_seq in zip(preds, refs):\n        pred_ents = extract_entities(pred_seq)\n        true_ents = extract_entities(ref_seq)\n        matched = set()\n\n        for pred_ent in pred_ents:\n            match_found = False\n            for i, true_ent in enumerate(true_ents):\n                if i in matched:\n                    continue\n                if relaxed_match(pred_ent, true_ent):\n                    tp += 1\n                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n                    matched.add(i)\n                    match_found = True\n                    break\n            if not match_found:\n                fp += 1\n                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n\n        for i, true_ent in enumerate(true_ents):\n            if i not in matched:\n                fn += 1\n                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n\n    precision = tp / (tp + fp + 1e-10)\n    recall    = tp / (tp + fn + 1e-10)\n    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n\n    print(\"\\n Relaxed Per-label HPO_TERM classification report:\")\n    for label, m in label_metrics.items():\n        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n        print(f\" {label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n# ----------- Step 4: 清除 -100 Padding ----------\nfiltered_preds = []\nfiltered_labels = []\n\nfor pred_seq, label_seq in zip(preds, labels):\n    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n    filtered_label = [l for l in label_seq if l != -100]\n    filtered_preds.append(filtered_pred)\n    filtered_labels.append(filtered_label)\n\n# ----------- Step 5: 修复结构（BIO） -----------\ndef clean_prediction_structure(labels):\n    \"\"\"修复孤立 I-、B-O-B 结构\"\"\"\n    cleaned = []\n    prev = \"O\"\n    for i, label in enumerate(labels):\n        if label.startswith(\"I-\") and prev == \"O\":\n            label = \"B-\" + label[2:]\n        if label == \"O\" and i+2 < len(labels) and labels[i+1].startswith(\"B-\") and labels[i+2].startswith(\"I-\"):\n            label = \"I-\" + labels[i+1][2:]\n        cleaned.append(label)\n        prev = label\n    return cleaned\n\ndef fix_illegal_I(labels):\n    \"\"\"修复 I- 前不是 B- 或 I- 的非法结构\"\"\"\n    fixed = []\n    prev_type = \"O\"\n    for label in labels:\n        if label.startswith(\"I-\"):\n            if prev_type != label[2:]:\n                label = \"B-\" + label[2:]\n        fixed.append(label)\n        if label.startswith(\"B-\"):\n            prev_type = label[2:]\n        elif label.startswith(\"I-\"):\n            pass\n        else:\n            prev_type = \"O\"\n    return fixed\n\ndef clean_and_fix_prediction_sequence(label_ids):\n    \"\"\"统一修复：结构 + I-合法性\"\"\"\n    labels = [id2label.get(lid, \"O\") for lid in label_ids]\n    labels = clean_prediction_structure(labels)\n    labels = fix_illegal_I(labels)\n    return [label2id.get(l, 0) for l in labels]\n\n# ----------- Step 6: 应用修复并评估 ----------\nfiltered_preds_cleaned = [clean_and_fix_prediction_sequence(seq) for seq in filtered_preds]\n\nprint(\"\\n Running relaxed evaluation on test set (HPO_TERM only, with structure repair)...\")\nrelaxed_metrics = relaxed_compute_metrics(filtered_preds_cleaned, filtered_labels)\nprint(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# ----------- Step 1: 提取 HPO_TERM 实体 spans -----------\ndef extract_entities(labels):\n    spans = []\n    start = None\n    current_label = None\n    for i, lab_id in enumerate(labels):\n        label = id2label.get(lab_id, \"O\")\n        if label.startswith(\"B-HPO_TERM\"):\n            if current_label:\n                spans.append((start, i - 1, current_label))\n            start = i\n            current_label = \"HPO_TERM\"\n        elif label.startswith(\"I-HPO_TERM\") and current_label:\n            continue\n        else:\n            if current_label:\n                spans.append((start, i - 1, current_label))\n                current_label = None\n                start = None\n    if current_label:\n        spans.append((start, len(labels) - 1, current_label))\n    return spans\n\n# ----------- Step 2: IOU计算 & Relaxed匹配 -----------\ndef iou(a, b):\n    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n    return inter / union\n\ndef relaxed_match(pred_span, true_span):\n    ps, pe, plabel = pred_span\n    ts, te, tlabel = true_span\n    if plabel != tlabel:\n        return False\n    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n        return True\n    if iou((ps, pe), (ts, te)) >= 0.4:\n        return True\n    return False\n\n# ----------- Step 3: Relaxed Evaluation Metric ----------\ndef relaxed_compute_metrics(preds, refs):\n    tp, fp, fn = 0, 0, 0\n    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n\n    for pred_seq, ref_seq in zip(preds, refs):\n        pred_ents = extract_entities(pred_seq)\n        true_ents = extract_entities(ref_seq)\n        matched = set()\n\n        for pred_ent in pred_ents:\n            match_found = False\n            for i, true_ent in enumerate(true_ents):\n                if i in matched:\n                    continue\n                if relaxed_match(pred_ent, true_ent):\n                    tp += 1\n                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n                    matched.add(i)\n                    match_found = True\n                    break\n            if not match_found:\n                fp += 1\n                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n\n        for i, true_ent in enumerate(true_ents):\n            if i not in matched:\n                fn += 1\n                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n\n    precision = tp / (tp + fp + 1e-10)\n    recall    = tp / (tp + fn + 1e-10)\n    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n\n    print(\"\\n Relaxed Per-label HPO_TERM classification report:\")\n    for label, m in label_metrics.items():\n        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n        print(f\"{label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n# ----------- Step 4: 清除 -100 Padding ----------\nfiltered_preds = []\nfiltered_labels = []\n\nfor pred_seq, label_seq in zip(preds, labels):\n    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n    filtered_label = [l for l in label_seq if l != -100]\n    filtered_preds.append(filtered_pred)\n    filtered_labels.append(filtered_label)\n\n# ----------- Step 5: 直接评估，无结构修复 ----------\nprint(\"\\n Running relaxed evaluation on test set (HPO_TERM only, no structure repair)...\")\nrelaxed_metrics = relaxed_compute_metrics(filtered_preds, filtered_labels)\nprint(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"加银数据","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom collections import Counter, defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoTokenizer\n\n# -------------------\n# Constants & Paths\n# -------------------\nFILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\nDIR_SILVER  = Path(\"/kaggle/input/hpo-only\")\nOUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_BIO = OUT_DIR / \"train.jsonl\"\nDEV_BIO   = OUT_DIR / \"dev.jsonl\"\nTEST_BIO  = OUT_DIR / \"test.jsonl\"\n\nENTITY_TYPES = {\n    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n}\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    use_fast=True\n)\n\n# -------------------\n# Utility Functions\n# -------------------\ndef iter_jsonl(path: Path):\n    with path.open(\"r\", encoding=\"utf-8\") as fh:\n        for line in fh:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                yield json.loads(line)\n            except json.JSONDecodeError:\n                continue\n\ndef make_bio_labels(spans, enc):\n    tokens   = enc.tokens()\n    offsets  = enc[\"offset_mapping\"]\n    word_ids = enc.word_ids()\n    tags     = [\"O\"] * len(tokens)\n    span_to_tokens = []\n\n    for sp in spans:\n        s, e, typ = sp[\"start\"], sp[\"end\"], sp[\"label\"]\n        idxs = [\n            i for i, (b, t) in enumerate(offsets)\n            if not (t <= s or b >= e)\n        ]\n        span_to_tokens.append(idxs)\n        if not idxs:\n            continue\n        tags[idxs[0]] = f\"B-{typ}\"\n        for i in idxs[1:]:\n            tags[i] = f\"I-{typ}\"\n\n    for idxs in span_to_tokens:\n        if len(idxs) == 1:\n            tags[idxs[0]] = tags[idxs[0]].replace(\"I-\", \"B-\")\n\n    prev_wid = None\n    for i, wid in enumerate(word_ids):\n        if (wid is not None and wid == prev_wid\n                and tags[i] == \"O\"\n                and tags[i - 1].startswith((\"B-\", \"I-\"))):\n            tags[i] = \"I-\" + tags[i - 1][2:]\n        prev_wid = wid\n\n    return tags\n\ndef record_to_bio(rec):\n    text  = rec.get(\"text\", \"\")\n    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n    if not spans:\n        return None\n    enc = tokenizer(\n        text,\n        add_special_tokens=False,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=512\n    )\n    return {\n        \"tokens\": enc.tokens(),\n        \"labels\": make_bio_labels(spans, enc)\n    }\n\ndef dump_jsonl(path: Path, data):\n    with path.open(\"w\", encoding=\"utf-8\") as fh:\n        for obj in data:\n            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n\ndef load_extra_bio(path: Path):\n    extra = []\n    for rec in iter_jsonl(path):\n        bio = record_to_bio(rec)\n        if bio:\n            extra.append(bio)\n    return extra\n\n# -------------------\n# Step 1: Load and convert gold data\n# -------------------\nprint(\">> Loading gold data …\")\nmerged_bio = [\n    bio for rec in iter_jsonl(FILE_MERGED)\n    if (bio := record_to_bio(rec)) is not None\n]\nprint(f\"Total valid records in gold: {len(merged_bio)}\")\n\n# -------------------\n# Step 2: Random split gold data\n# -------------------\n# 第一步：先拿出 20% 给 test\ntrain_dev, test_set = train_test_split(\n    merged_bio,\n    test_size=0.20,\n    random_state=42\n)\n\n# 第二步：再把剩下的 80% 中，25% 给 dev ⇒ 0.25×80% = 20%\ntrain_set, dev_set = train_test_split(\n    train_dev,\n    test_size=0.25,\n    random_state=42\n)\n\nprint(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n\n# -------------------\n# Step 3: Add silver data to train set\n# -------------------\nextra_train = []\nif DIR_SILVER.exists():\n    print(\">> Loading silver data from hpo-only/\")\n    for jf in sorted(DIR_SILVER.glob(\"*.jsonl\")):\n        print(f\"  - {jf.name}\")\n        extra_train.extend(load_extra_bio(jf))\nelse:\n    print(\">> Silver data directory not found.\")\n\ntrain_final = train_set + extra_train\nprint(f\"Final train size: {len(train_final)} (including {len(extra_train)} silver records)\")\n\n# -------------------\n# Step 4: Save to disk\n# -------------------\ndump_jsonl(TRAIN_BIO, train_final)\ndump_jsonl(DEV_BIO, dev_set)\ndump_jsonl(TEST_BIO, test_set)\n\nprint(f\"\\nSaved to:\")\nprint(f\"  ➜ {TRAIN_BIO}\")\nprint(f\"  ➜ {DEV_BIO}\")\nprint(f\"  ➜ {TEST_BIO}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    DataCollatorForTokenClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport evaluate\n\n# 1. Load BIO datasets\nBIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n\ndef load_jsonl(path: Path):\n    with path.open(encoding=\"utf-8\") as f:\n        return [json.loads(line) for line in f if line.strip()]\n\n# 1.1 过滤非 HPO_TERM 标签为 'O'\ndef keep_only_hpo_labels(example):\n    example[\"labels\"] = [\n        lab if lab.endswith(\"HPO_TERM\") else \"O\"\n        for lab in example[\"labels\"]\n    ]\n    return example\n\ntrain_examples = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"train.jsonl\")]\ndev_examples   = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"dev.jsonl\")]\ntest_examples  = [keep_only_hpo_labels(ex) for ex in load_jsonl(BIO_DIR / \"test.jsonl\")]\n\nds_splits = DatasetDict({\n    \"train\":      Dataset.from_list(train_examples),\n    \"validation\": Dataset.from_list(dev_examples),\n    \"test\":       Dataset.from_list(test_examples),\n})\nprint(\"Loaded dataset sizes:\", {k: len(v) for k, v in ds_splits.items()})\n\n# 2. Tokenizer & label mapping\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    use_fast=True,\n)\n\n# 2.1 只保留 ['B-HPO_TERM', 'I-HPO_TERM', 'O']\nunique_labels = sorted({lab for ex in train_examples + dev_examples + test_examples\n                        for lab in ex[\"labels\"]})\nlabel2id = {lab: i for i, lab in enumerate(unique_labels)}\nid2label = {i: lab for lab, i in label2id.items()}\n\ndef tokenize_and_align_labels(ex):\n    enc = tokenizer(\n        ex[\"tokens\"],\n        is_split_into_words=True,\n        truncation=True,\n        max_length=512,\n        return_attention_mask=True,\n    )\n    enc[\"labels\"] = [label2id[l] for l in ex[\"labels\"]]\n    return enc\n\nds_splits = ds_splits.map(\n    tokenize_and_align_labels,\n    batched=False,\n    remove_columns=[\"tokens\", \"labels\"],\n)\n\n# 3. Model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    num_labels=len(unique_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True,\n)\n\n# 4. Metrics\nseqeval = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    refs = p.label_ids\n    true_labels = [\n        [id2label[lid] for lid in seq if lid != -100] for seq in refs\n    ]\n    pred_labels = [\n        [id2label[pid] for pid, lid in zip(pred_seq, ref_seq) if lid != -100]\n        for pred_seq, ref_seq in zip(preds, refs)\n    ]\n    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n    return {\n        \"overall_precision\": result[\"overall_precision\"],\n        \"overall_recall\":    result[\"overall_recall\"],\n        \"overall_f1\":        result[\"overall_f1\"],\n        \"overall_accuracy\":  result[\"overall_accuracy\"],\n    }\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\n# 5. Training arguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"ner_pubmedbert\",\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"overall_f1\",\n    greater_is_better=True,\n    report_to=[\"none\"],\n    save_total_limit=1,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_splits[\"train\"],\n    eval_dataset=ds_splits[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# 6. Train and evaluate\ntrainer.train()\ntrainer.evaluate()\n\n# 7. Predict on test set\ntest_metrics = trainer.predict(ds_splits[\"test\"]).metrics\nprint(\"Test set metrics:\", test_metrics)\npredictions, labels, _ = trainer.predict(ds_splits[\"test\"])\npreds = predictions.argmax(-1)\n\ntrue_labels = [\n    [id2label[label_id] for label_id in seq if label_id != -100]\n    for seq in labels\n]\npred_labels = [\n    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n    for pred_seq, label_seq in zip(preds, labels)\n]\n\ndetailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n\n# 8. Only show HPO_TERM in per-label report\nprint(\"\\n HPO_TERM classification report:\")\nfor label, metrics in detailed_result.items():\n    if label.startswith(\"overall_\"):\n        continue\n    if label != \"HPO_TERM\":\n        continue\n    print(f\" {label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from evaluate import load\n\n# 预测\npredictions, labels, _ = trainer.predict(ds_splits[\"test\"])\npreds = predictions.argmax(-1)\n\n# 还原标签ID为标签名（跳过 -100，即 ignore_index）\ntrue_labels = [\n    [id2label[label_id] for label_id in seq if label_id != -100]\n    for seq in labels\n]\npred_labels = [\n    [id2label[pred_id] for pred_id, label_id in zip(pred_seq, label_seq) if label_id != -100]\n    for pred_seq, label_seq in zip(preds, labels)\n]\n\n# 使用 seqeval 计算所有标签评估结果\nseqeval = load(\"seqeval\")\ndetailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n\n# 打印 overall F1（可选）\nprint(f\"\\nOverall F1 score: {detailed_result.get('overall_f1', 0):.3f}\")\n\n# 只输出 HPO_TERM 的结果\nprint(\"\\n HPO_TERM classification report:\")\nhpo_metrics = detailed_result.get(\"HPO_TERM\")\nif hpo_metrics:\n    print(f\" {'HPO_TERM':20} | Precision: {hpo_metrics['precision']:.3f} | Recall: {hpo_metrics['recall']:.3f} | F1: {hpo_metrics['f1']:.3f}\")\nelse:\n    print(\"No HPO_TERM entities found in predictions.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# ----------- Step 1: 提取 HPO_TERM 实体 spans -----------\ndef extract_entities(labels):\n    spans = []\n    start = None\n    current_label = None\n    for i, lab_id in enumerate(labels):\n        label = id2label.get(lab_id, \"O\")\n        if label.startswith(\"B-HPO_TERM\"):\n            if current_label:\n                spans.append((start, i - 1, current_label))\n            start = i\n            current_label = \"HPO_TERM\"\n        elif label.startswith(\"I-HPO_TERM\") and current_label:\n            continue\n        else:\n            if current_label:\n                spans.append((start, i - 1, current_label))\n                current_label = None\n                start = None\n    if current_label:\n        spans.append((start, len(labels) - 1, current_label))\n    return spans\n\n# ----------- Step 2: IOU计算 & Relaxed匹配 -----------\ndef iou(a, b):\n    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n    return inter / union\n\ndef relaxed_match(pred_span, true_span):\n    ps, pe, plabel = pred_span\n    ts, te, tlabel = true_span\n    if plabel != tlabel:\n        return False\n    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n        return True\n    if iou((ps, pe), (ts, te)) >= 0.4:\n        return True\n    return False\n\n# ----------- Step 3: Relaxed Evaluation Metric ----------\ndef relaxed_compute_metrics(preds, refs):\n    tp, fp, fn = 0, 0, 0\n    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n\n    for pred_seq, ref_seq in zip(preds, refs):\n        pred_ents = extract_entities(pred_seq)\n        true_ents = extract_entities(ref_seq)\n        matched = set()\n\n        for pred_ent in pred_ents:\n            match_found = False\n            for i, true_ent in enumerate(true_ents):\n                if i in matched:\n                    continue\n                if relaxed_match(pred_ent, true_ent):\n                    tp += 1\n                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n                    matched.add(i)\n                    match_found = True\n                    break\n            if not match_found:\n                fp += 1\n                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n\n        for i, true_ent in enumerate(true_ents):\n            if i not in matched:\n                fn += 1\n                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n\n    precision = tp / (tp + fp + 1e-10)\n    recall    = tp / (tp + fn + 1e-10)\n    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n\n    print(\"\\n Relaxed Per-label HPO_TERM classification report:\")\n    for label, m in label_metrics.items():\n        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n        print(f\" {label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n# ----------- Step 4: 清除 -100 Padding ----------\nfiltered_preds = []\nfiltered_labels = []\n\nfor pred_seq, label_seq in zip(preds, labels):\n    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n    filtered_label = [l for l in label_seq if l != -100]\n    filtered_preds.append(filtered_pred)\n    filtered_labels.append(filtered_label)\n\n# ----------- Step 5: 修复结构（BIO） -----------\ndef clean_prediction_structure(labels):\n    \"\"\"修复孤立 I-、B-O-B 结构\"\"\"\n    cleaned = []\n    prev = \"O\"\n    for i, label in enumerate(labels):\n        if label.startswith(\"I-\") and prev == \"O\":\n            label = \"B-\" + label[2:]\n        if label == \"O\" and i+2 < len(labels) and labels[i+1].startswith(\"B-\") and labels[i+2].startswith(\"I-\"):\n            label = \"I-\" + labels[i+1][2:]\n        cleaned.append(label)\n        prev = label\n    return cleaned\n\ndef fix_illegal_I(labels):\n    \"\"\"修复 I- 前不是 B- 或 I- 的非法结构\"\"\"\n    fixed = []\n    prev_type = \"O\"\n    for label in labels:\n        if label.startswith(\"I-\"):\n            if prev_type != label[2:]:\n                label = \"B-\" + label[2:]\n        fixed.append(label)\n        if label.startswith(\"B-\"):\n            prev_type = label[2:]\n        elif label.startswith(\"I-\"):\n            pass\n        else:\n            prev_type = \"O\"\n    return fixed\n\ndef clean_and_fix_prediction_sequence(label_ids):\n    \"\"\"统一修复：结构 + I-合法性\"\"\"\n    labels = [id2label.get(lid, \"O\") for lid in label_ids]\n    labels = clean_prediction_structure(labels)\n    labels = fix_illegal_I(labels)\n    return [label2id.get(l, 0) for l in labels]\n\n# ----------- Step 6: 应用修复并评估 ----------\nfiltered_preds_cleaned = [clean_and_fix_prediction_sequence(seq) for seq in filtered_preds]\n\nprint(\"\\n Running relaxed evaluation on test set (HPO_TERM only)...\")\nrelaxed_metrics = relaxed_compute_metrics(filtered_preds_cleaned, filtered_labels)\nprint(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\n\n# ----------- Step 1: 提取 HPO_TERM 实体 spans -----------\ndef extract_entities(labels):\n    spans = []\n    start = None\n    current_label = None\n    for i, lab_id in enumerate(labels):\n        label = id2label.get(lab_id, \"O\")\n        if label.startswith(\"B-HPO_TERM\"):\n            if current_label:\n                spans.append((start, i - 1, current_label))\n            start = i\n            current_label = \"HPO_TERM\"\n        elif label.startswith(\"I-HPO_TERM\") and current_label:\n            continue\n        else:\n            if current_label:\n                spans.append((start, i - 1, current_label))\n                current_label = None\n                start = None\n    if current_label:\n        spans.append((start, len(labels) - 1, current_label))\n    return spans\n\n# ----------- Step 2: IOU计算 & Relaxed匹配 -----------\ndef iou(a, b):\n    inter = max(0, min(a[1], b[1]) - max(a[0], b[0]) + 1)\n    union = max(a[1], b[1]) - min(a[0], b[0]) + 1\n    return inter / union\n\ndef relaxed_match(pred_span, true_span):\n    ps, pe, plabel = pred_span\n    ts, te, tlabel = true_span\n    if plabel != tlabel:\n        return False\n    if abs(ps - ts) <= 4 and abs(pe - te) <= 4:\n        return True\n    if iou((ps, pe), (ts, te)) >= 0.4:\n        return True\n    return False\n\n# ----------- Step 3: Relaxed Evaluation Metric ----------\ndef relaxed_compute_metrics(preds, refs):\n    tp, fp, fn = 0, 0, 0\n    label_metrics = defaultdict(lambda: {\"tp\": 0, \"fp\": 0, \"fn\": 0})\n\n    for pred_seq, ref_seq in zip(preds, refs):\n        pred_ents = extract_entities(pred_seq)\n        true_ents = extract_entities(ref_seq)\n        matched = set()\n\n        for pred_ent in pred_ents:\n            match_found = False\n            for i, true_ent in enumerate(true_ents):\n                if i in matched:\n                    continue\n                if relaxed_match(pred_ent, true_ent):\n                    tp += 1\n                    label_metrics[\"HPO_TERM\"][\"tp\"] += 1\n                    matched.add(i)\n                    match_found = True\n                    break\n            if not match_found:\n                fp += 1\n                label_metrics[\"HPO_TERM\"][\"fp\"] += 1\n\n        for i, true_ent in enumerate(true_ents):\n            if i not in matched:\n                fn += 1\n                label_metrics[\"HPO_TERM\"][\"fn\"] += 1\n\n    precision = tp / (tp + fp + 1e-10)\n    recall    = tp / (tp + fn + 1e-10)\n    f1        = 2 * precision * recall / (precision + recall + 1e-10)\n\n    print(\"\\nRelaxed Per-label HPO_TERM classification report:\")\n    for label, m in label_metrics.items():\n        lp = m[\"tp\"] / (m[\"tp\"] + m[\"fp\"] + 1e-10)\n        lr = m[\"tp\"] / (m[\"tp\"] + m[\"fn\"] + 1e-10)\n        lf1 = 2 * lp * lr / (lp + lr + 1e-10)\n        print(f\"{label:20} | Precision: {lp:.3f} | Recall: {lr:.3f} | F1: {lf1:.3f}\")\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n# ----------- Step 4: 清除 -100 Padding ----------\nfiltered_preds = []\nfiltered_labels = []\n\nfor pred_seq, label_seq in zip(preds, labels):\n    filtered_pred = [p for p, l in zip(pred_seq, label_seq) if l != -100]\n    filtered_label = [l for l in label_seq if l != -100]\n    filtered_preds.append(filtered_pred)\n    filtered_labels.append(filtered_label)\n\n# ----------- Step 5: 直接评估，无结构修复 ----------\nprint(\"\\n Running relaxed evaluation on test set...\")\nrelaxed_metrics = relaxed_compute_metrics(filtered_preds, filtered_labels)\nprint(\"\\n Relaxed HPO_TERM test set metrics:\", relaxed_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.save_model(\"ner_pubmedbert_saved_HPO\")\ntokenizer.save_pretrained(\"ner_pubmedbert_saved_HPO\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers obonet rapidfuzz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\nOUT_FILE  = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\nMAX_LENGTH = 512  # 模型最大 token 长度\nDEVICE = \"cuda:0\"\n\n# === Step 1: 读取测试数据 ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\nsentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n\n# === Step 2: 加载 tokenizer & model ===\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\nmodel.to(DEVICE)\nmodel.eval()\n\n# label map: id -> label string, e.g. \"B-HPO_TERM\", \"I-HPO_TERM\", \"O\"\nid2label = model.config.id2label\n\n# === Step 3: 构建 HPO ontology 映射 ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        text = syn.split('\"')[1]\n        hpo_map.setdefault(text.lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n    if matches:\n        return hpo_map[matches[0][0]][0]\n    return None\n\n# === Step 4: 对每句执行 NER ===\nnormalized_mentions = []\n\nfor idx, sentence in enumerate(sentences):\n    # Tokenize + truncate\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        return_tensors=\"pt\"\n    ).to(DEVICE)\n\n    with torch.no_grad():\n        outputs = model(**{k: encoding[k] for k in [\"input_ids\",\"attention_mask\"]})\n    logits = outputs.logits  # shape [1, seq_len, num_labels]\n    preds = logits.argmax(dim=-1)[0].cpu().tolist()  # [seq_len]\n    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()  # [(start,end),...]\n\n    # Extract contiguous HPO_TERM spans\n    span_start, span_end = None, None\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        if label == \"B-HPO_TERM\":\n            # start new span\n            span_start = offsets[i][0]\n            span_end = offsets[i][1]\n        elif label == \"I-HPO_TERM\" and span_start is not None:\n            # continue span\n            span_end = offsets[i][1]\n        else:\n            # label is \"O\" or a new B- or outside; close existing span\n            if span_start is not None:\n                mention_text = sentence[span_start:span_end]\n                hpo_id = normalize_mention(mention_text)\n                normalized_mentions.append({\n                    \"sentence_index\": idx,\n                    \"sentence\":       sentence,\n                    \"mention\":        mention_text,\n                    \"span\":           (span_start, span_end),\n                    \"hpo_id\":         hpo_id\n                })\n                span_start, span_end = None, None\n            # no action on O or other B-\n\n    # if sentence ends with a span open\n    if span_start is not None:\n        mention_text = sentence[span_start:span_end]\n        hpo_id = normalize_mention(mention_text)\n        normalized_mentions.append({\n            \"sentence_index\": idx,\n            \"sentence\":       sentence,\n            \"mention\":        mention_text,\n            \"span\":           (span_start, span_end),\n            \"hpo_id\":         hpo_id\n        })\n\n# === Step 5: 输出统计 ===\ntotal = len(normalized_mentions)\nmapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\nprint(f\"Total mentions: {total}\")\nprint(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\nprint(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n\n# === Step 6: 保存为 JSONL ===\nwith OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n    for rec in normalized_mentions:\n        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Normalized results saved to: {OUT_FILE.resolve()}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\nOUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\nMAX_LENGTH = 512         # 模型最大 token 长度\nDEVICE     = \"cuda:0\"\n\n# === Utility: 清洗函数（去掉数字、标点、##子词标记） ===\ndef clean_text(text: str) -> str:\n    # 去除文献引用 [24, 25]、孤立数字、百分比\n    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n    text = re.sub(r\"\\d+%?\", \"\", text)\n    # 去掉子词标记\n    text = text.replace(\"##\", \"\")\n    # （可选）去掉多余空格\n    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n\n# === Utility: 噪声过滤 ===\ndef is_noise(mention: str) -> bool:\n    # 纯数字或百分号\n    if re.fullmatch(r\"\\d+%?\", mention):\n        return True\n    # 纯标点\n    if re.fullmatch(r\"[^\\w\\s]+\", mention):\n        return True\n    return False\n\n# === Step 1: 读取测试数据 ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n\n# === Step 2: 加载 tokenizer & model ===\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_DIR, use_fast=True, local_files_only=True\n)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_DIR, local_files_only=True\n).to(DEVICE)\nmodel.eval()\n\n# id -> label 映射，如 \"B-HPO_TERM\", \"I-HPO_TERM\", \"O\"\nid2label = model.config.id2label\n\n# === Step 3: 构建 HPO ontology 映射 ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph   = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        text = syn.split('\"')[1]\n        hpo_map.setdefault(text.lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n    return hpo_map[matches[0][0]][0] if matches else None\n\n# === Step 4: 对每一句文本清洗 + NER 抽取 ===\nnormalized_mentions = []\n\nfor idx, orig in enumerate(orig_sentences):\n    # 4.1 清洗\n    sentence = clean_text(orig)\n\n    # 4.2 分词 & 截断 & 编码\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        return_tensors=\"pt\"\n    ).to(DEVICE)\n\n    # 4.3 模型推理\n    with torch.no_grad():\n        outputs = model(\n            input_ids=encoding[\"input_ids\"],\n            attention_mask=encoding[\"attention_mask\"]\n        )\n    logits  = outputs.logits[0]                  # [seq_len, num_labels]\n    preds   = logits.argmax(dim=-1).cpu().tolist()\n    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n\n    # 4.4 提取连续 HPO_TERM span\n    span_start = span_end = None\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        if label == \"B-HPO_TERM\":\n            span_start, span_end = offsets[i]\n        elif label == \"I-HPO_TERM\" and span_start is not None:\n            span_end = offsets[i][1]\n        else:\n            if span_start is not None:\n                mention = sentence[span_start:span_end]\n                # 4.5 后处理：过滤噪声\n                if not is_noise(mention):\n                    normalized_mentions.append({\n                        \"sentence_index\": idx,\n                        \"sentence\":       sentence,\n                        \"mention\":        mention,\n                        \"span\":           (span_start, span_end),\n                        \"hpo_id\":         normalize_mention(mention)\n                    })\n                span_start = span_end = None\n\n    # 末尾若仍有未闭合 span\n    if span_start is not None:\n        mention = sentence[span_start:span_end]\n        if not is_noise(mention):\n            normalized_mentions.append({\n                \"sentence_index\": idx,\n                \"sentence\":       sentence,\n                \"mention\":        mention,\n                \"span\":           (span_start, span_end),\n                \"hpo_id\":         normalize_mention(mention)\n            })\n\n# === Step 5: 统计覆盖率 & 保存结果 ===\ntotal  = len(normalized_mentions)\nmapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\nprint(f\"Total mentions: {total}\")\nprint(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\nprint(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n\nwith OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n    for rec in normalized_mentions:\n        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Results saved to {OUT_FILE.resolve()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\nOUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\nMAX_LENGTH = 512         # 模型最大 token 长度\nDEVICE     = \"cuda:0\"\n\n# === Utility: 清洗函数（去掉数字、标点、##子词标记、[UNK]） ===\ndef clean_text(text: str) -> str:\n    text = text.replace(\"[UNK]\", \" \")\n    # 去除文献引用 [24, 25]\n    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n    # 去除百分比和孤立数字\n    text = re.sub(r\"\\d+%?\", \"\", text)\n    # 去掉子词标记\n    text = text.replace(\"##\", \"\")\n    # 合并多余空格\n    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n\n# === Utility: 噪声过滤 ===\ndef is_noise(mention: str) -> bool:\n    # 纯数字或百分号\n    if re.fullmatch(r\"\\d+%?\", mention):\n        return True\n    # 纯标点\n    if re.fullmatch(r\"[^\\w\\s]+\", mention):\n        return True\n    # 过短\n    if len(mention.strip()) < 1:\n        return True\n    return False\n\n# === Step 1: 读取测试数据 ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n\n# === Step 2: 加载 tokenizer & model ===\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_DIR, use_fast=True, local_files_only=True\n)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_DIR, local_files_only=True\n).to(DEVICE)\nmodel.eval()\n\n# id -> label 映射，如 \"B-HPO_TERM\", \"I-HPO_TERM\", \"O\"\nid2label = model.config.id2label\n\n# === Step 3: 构建 HPO ontology 映射 ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph   = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        text = syn.split('\"')[1]\n        hpo_map.setdefault(text.lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n    return hpo_map[matches[0][0]][0] if matches else None\n\n# === Step 4: 对每一句文本清洗 + NER 抽取 ===\nnormalized_mentions = []\n\nfor idx, orig in enumerate(orig_sentences):\n    # 4.1 清洗\n    sentence = clean_text(orig)\n\n    # 4.2 分词 & 截断 & 编码\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        return_tensors=\"pt\"\n    ).to(DEVICE)\n\n    # 4.3 模型推理\n    with torch.no_grad():\n        outputs = model(\n            input_ids=encoding[\"input_ids\"],\n            attention_mask=encoding[\"attention_mask\"]\n        )\n    logits  = outputs.logits[0]                  # [seq_len, num_labels]\n    preds   = logits.argmax(dim=-1).cpu().tolist()\n    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n\n    # 4.4 提取连续 HPO_TERM span\n    span_start = span_end = None\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        if label == \"B-HPO_TERM\":\n            span_start, span_end = offsets[i]\n        elif label == \"I-HPO_TERM\" and span_start is not None:\n            span_end = offsets[i][1]\n        else:\n            if span_start is not None:\n                mention = sentence[span_start:span_end]\n                # 4.5 后处理：过滤噪声\n                if not is_noise(mention):\n                    normalized_mentions.append({\n                        \"sentence_index\": idx,\n                        \"sentence\":       sentence,\n                        \"mention\":        mention,\n                        \"span\":           (span_start, span_end),\n                        \"hpo_id\":         normalize_mention(mention)\n                    })\n                span_start = span_end = None\n\n    # 末尾若仍有未闭合 span\n    if span_start is not None:\n        mention = sentence[span_start:span_end]\n        if not is_noise(mention):\n            normalized_mentions.append({\n                \"sentence_index\": idx,\n                \"sentence\":       sentence,\n                \"mention\":        mention,\n                \"span\":           (span_start, span_end),\n                \"hpo_id\":         normalize_mention(mention)\n            })\n\n# === Step 5: 统计覆盖率 & 保存结果 ===\ntotal  = len(normalized_mentions)\nmapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\nprint(f\"Total mentions: {total}\")\nprint(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\nprint(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n\nwith OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n    for rec in normalized_mentions:\n        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Results saved to {OUT_FILE.resolve()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\nOUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\nMAX_LENGTH = 512\nDEVICE     = \"cuda:0\"\n\n# === 清洗文本 ===\ndef clean_text(text: str) -> str:\n    text = text.replace(\"[UNK]\", \" \")\n    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n    text = re.sub(r\"\\d+%?\", \"\", text)\n    text = text.replace(\"##\", \"\")\n    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n\n# === 判定是否是噪声 ===\ndef is_noise(mention: str) -> bool:\n    if re.fullmatch(r\"\\d+%?\", mention): return True\n    if re.fullmatch(r\"[^\\w\\s]+\", mention): return True\n    if len(mention.strip()) < 3: return True\n    return False\n\n# === Step 1: 读取测试数据 ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n\n# === Step 2: 模型和Tokenizer加载 ===\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\nmodel.eval()\nid2label = model.config.id2label\n\n# === Step 3: HPO 词典加载 ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        text = syn.split('\"')[1]\n        hpo_map.setdefault(text.lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=85)\n    if matches:\n        return hpo_map[matches[0][0]][0]\n    return None\n\n# === Step 4: NER处理 ===\nnormalized_mentions = []\n\nfor idx, orig in enumerate(orig_sentences):\n    sentence = clean_text(orig)\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=MAX_LENGTH,\n        is_split_into_words=False\n    )\n\n    # ❗️移除 offset_mapping，避免传入模型\n    offset_mapping = encoding.pop(\"offset_mapping\")\n    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n\n    with torch.no_grad():\n        outputs = model(**encoding)\n    logits = outputs.logits[0]  # shape: [seq_len, num_labels]\n    probs  = torch.softmax(logits, dim=-1)\n    preds  = probs.argmax(dim=-1).cpu().tolist()\n    scores = probs.max(dim=-1).values.cpu().tolist()\n    offsets = offset_mapping[0].tolist()\n    tokens  = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n\n    current_mention = \"\"\n    current_start = None\n    current_score = []\n\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        token = tokens[i]\n        offset = offsets[i]\n        score = scores[i]\n\n        if offset[0] == offset[1]:  # special tokens like [CLS], [SEP]\n            continue\n        if label == \"B-HPO_TERM\":\n            if current_mention:\n                mention = current_mention.strip()\n                hpo_id = normalize_mention(mention)\n                avg_score = sum(current_score)/len(current_score) if current_score else 0\n                if hpo_id and not is_noise(mention) and avg_score > 0.6:\n                    normalized_mentions.append({\n                        \"sentence_index\": idx,\n                        \"sentence\":       sentence,\n                        \"mention\":        mention,\n                        \"span\":           (current_start, offset[0]),\n                        \"hpo_id\":         hpo_id\n                    })\n            current_mention = sentence[offset[0]:offset[1]]\n            current_start = offset[0]\n            current_score = [score]\n        elif label == \"I-HPO_TERM\" and current_mention:\n            current_mention += sentence[offset[0]:offset[1]]\n            current_score.append(score)\n        else:\n            if current_mention:\n                mention = current_mention.strip()\n                hpo_id = normalize_mention(mention)\n                avg_score = sum(current_score)/len(current_score) if current_score else 0\n                if hpo_id and not is_noise(mention) and avg_score > 0.6:\n                    normalized_mentions.append({\n                        \"sentence_index\": idx,\n                        \"sentence\":       sentence,\n                        \"mention\":        mention,\n                        \"span\":           (current_start, offset[0]),\n                        \"hpo_id\":         hpo_id\n                    })\n            current_mention = \"\"\n            current_score = []\n            current_start = None\n\n    # 收尾\n    if current_mention:\n        mention = current_mention.strip()\n        hpo_id = normalize_mention(mention)\n        avg_score = sum(current_score)/len(current_score) if current_score else 0\n        if hpo_id and not is_noise(mention) and avg_score > 0.6:\n            normalized_mentions.append({\n                \"sentence_index\": idx,\n                \"sentence\":       sentence,\n                \"mention\":        mention,\n                \"span\":           (current_start, len(sentence)),\n                \"hpo_id\":         hpo_id\n            })\n\n# === Step 5: 输出统计与保存 ===\ntotal  = len(normalized_mentions)\nmapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\nprint(f\"Total mentions: {total}\")\nprint(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\nprint(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n\nwith OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n    for rec in normalized_mentions:\n        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Results saved to {OUT_FILE.resolve()}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install nltk obonet rapidfuzz transformers torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import nltk\nnltk.download('punkt'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE = Path(\"/kaggle/working/bio_outputs/test_text_only.jsonl\")\nMAX_LENGTH = 512\nDEVICE = \"cuda:0\"\n\n# === Utility: 清洗函数（去掉 [UNK]、子词标记、文献引用、孤立数字）===\ndef clean_text(text: str) -> str:\n    text = text.replace(\"[UNK]\", \" \")\n    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n    text = re.sub(r\"\\d+%?\", \"\", text)\n    text = text.replace(\"##\", \"\")\n    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n\n# === Utility: 更严格的噪声过滤（防粘连、无意义单词）===\ndef is_noise(mention: str) -> bool:\n    mention = mention.strip()\n    if not mention:\n        return True\n    if re.fullmatch(r\"\\d+%?\", mention):\n        return True\n    if re.fullmatch(r\"[^\\w\\s]+\", mention):\n        return True\n    if len(mention) < 3:\n        return True\n    if not re.search(r\"[aeiou]\", mention.lower()):\n        return True\n    if len(mention) > 25 and \" \" not in mention:\n        return True\n    blacklist = {\"showed\", \"found\", \"revealed\", \"including\", \"video\", \"fig\", \"fig.\", \"information\"}\n    if mention.lower() in blacklist:\n        return True\n    return False\n\n# === Step 1: Load raw sentence text ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [ex[\"text\"] for ex in test_data]\n\n# === Step 2: Load model and tokenizer ===\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\nmodel.eval()\nid2label = model.config.id2label\n\n# === Step 3: Build HPO dictionary from hp.obo ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        match = re.search(r'\"(.+?)\"', syn)\n        if match:\n            hpo_map.setdefault(match.group(1).lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    match = process.extractOne(key, hpo_map.keys(), score_cutoff=85)\n    if match:\n        return hpo_map[match[0]][0]\n    return None\n\n# === Step 4: Run NER + Normalize ===\nmapped_mentions = []\nunmapped_mentions = []\n\nfor idx, orig in enumerate(orig_sentences):\n    sentence = clean_text(orig)\n\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=MAX_LENGTH,\n        is_split_into_words=False\n    )\n    offset_mapping = encoding.pop(\"offset_mapping\")[0].tolist()\n    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n\n    with torch.no_grad():\n        outputs = model(**encoding)\n    preds = outputs.logits.argmax(dim=-1)[0].cpu().tolist()\n\n    current_offsets = []\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        start, end = offset_mapping[i]\n        if start == end:\n            continue\n        if label == \"B-HPO_TERM\":\n            if current_offsets:\n                spans = current_offsets\n                mention = \" \".join([sentence[s:e] for s, e in spans]).strip()\n                if not is_noise(mention):\n                    hpo_id = normalize_mention(mention)\n                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n            current_offsets = [(start, end)]\n        elif label == \"I-HPO_TERM\" and current_offsets:\n            current_offsets.append((start, end))\n        else:\n            if current_offsets:\n                spans = current_offsets\n                mention = \" \".join([sentence[s:e] for s, e in spans]).strip()\n                if not is_noise(mention):\n                    hpo_id = normalize_mention(mention)\n                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n            current_offsets = []\n\n    # Last one\n    if current_offsets:\n        spans = current_offsets\n        mention = \" \".join([sentence[s:e] for s, e in spans]).strip()\n        if not is_noise(mention):\n            hpo_id = normalize_mention(mention)\n            (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n\n# === Step 5: Output\nprint(f\"\\n✅ Mapped Mentions ({len(mapped_mentions)}):\")\nfor mention, hpo_id in mapped_mentions:\n    print(f\"{mention} --> {hpo_id}\")\n\nprint(f\"\\n❌ Unmapped Mentions ({len(unmapped_mentions)}):\")\nfor mention, _ in unmapped_mentions:\n    print(mention)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE = Path(\"/kaggle/working/bio_outputs/test_text_only.jsonl\")\nMAX_LENGTH = 512\nDEVICE = \"cuda:0\"\n\n# === Utility: 清洗函数（去掉 [UNK]、子词标记、文献引用、孤立数字）===\ndef clean_text(text: str) -> str:\n    text = text.replace(\"[UNK]\", \" \")\n    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n    text = re.sub(r\"\\d+%?\", \"\", text)\n    text = text.replace(\"##\", \"\")\n    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n\n# === Utility: 更严格的噪声过滤 ===\ndef is_noise(mention: str) -> bool:\n    mention = mention.strip()\n    if not mention:\n        return True\n    if re.fullmatch(r\"\\d+%?\", mention): return True\n    if re.fullmatch(r\"[^\\w\\s]+\", mention): return True\n    if len(mention) < 3: return True\n    if not re.search(r\"[aeiou]\", mention.lower()): return True\n    if len(mention) > 25 and \" \" not in mention: return True\n    if len(mention.split()) < 2 and len(mention) <= 5: return True\n\n    blacklist = {\n        \"showed\", \"found\", \"revealed\", \"including\", \"video\", \"fig\", \"fig.\",\n        \"information\", \"inserted\", \"chinese\", \"data\", \"further\", \"proband\", \"thereafter\"\n    }\n    if mention.lower().strip(\".\") in blacklist:\n        return True\n    return False\n\n# === Step 1: Load test data ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [ex[\"text\"] for ex in test_data]\n\n# === Step 2: Load model and tokenizer ===\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\nmodel.eval()\nid2label = model.config.id2label\n\n# === Step 3: Load HPO terms from hp.obo ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        match = re.search(r'\"(.+?)\"', syn)\n        if match:\n            hpo_map.setdefault(match.group(1).lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    match = process.extractOne(key, hpo_map.keys(), score_cutoff=85)\n    if match:\n        return hpo_map[match[0]][0]\n    return None\n\n# === Step 4: Run NER + Normalize ===\nmapped_mentions = []\nunmapped_mentions = []\n\nfor idx, orig in enumerate(orig_sentences):\n    sentence = clean_text(orig)\n\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        return_tensors=\"pt\",\n        truncation=True,\n        max_length=MAX_LENGTH,\n        is_split_into_words=False\n    )\n    offset_mapping = encoding.pop(\"offset_mapping\")[0].tolist()\n    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n\n    with torch.no_grad():\n        outputs = model(**encoding)\n    preds = outputs.logits.argmax(dim=-1)[0].cpu().tolist()\n\n    current_offsets = []\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        start, end = offset_mapping[i]\n        if start == end:\n            continue\n        if label == \"B-HPO_TERM\":\n            if current_offsets:\n                spans = current_offsets\n                mention = \" \".join([sentence[s:e] for s, e in spans])\n                mention = re.sub(r\"^[^\\w]+\", \"\", mention)\n                mention = re.sub(r\"[^\\w]+$\", \"\", mention)\n                mention = re.sub(r\"\\s{2,}\", \" \", mention).strip()\n                if not is_noise(mention):\n                    hpo_id = normalize_mention(mention)\n                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n            current_offsets = [(start, end)]\n        elif label == \"I-HPO_TERM\" and current_offsets:\n            current_offsets.append((start, end))\n        else:\n            if current_offsets:\n                spans = current_offsets\n                mention = \" \".join([sentence[s:e] for s, e in spans])\n                mention = re.sub(r\"^[^\\w]+\", \"\", mention)\n                mention = re.sub(r\"[^\\w]+$\", \"\", mention)\n                mention = re.sub(r\"\\s{2,}\", \" \", mention).strip()\n                if not is_noise(mention):\n                    hpo_id = normalize_mention(mention)\n                    (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n            current_offsets = []\n\n    # 最后一个 mention\n    if current_offsets:\n        spans = current_offsets\n        mention = \" \".join([sentence[s:e] for s, e in spans])\n        mention = re.sub(r\"^[^\\w]+\", \"\", mention)\n        mention = re.sub(r\"[^\\w]+$\", \"\", mention)\n        mention = re.sub(r\"\\s{2,}\", \" \", mention).strip()\n        if not is_noise(mention):\n            hpo_id = normalize_mention(mention)\n            (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n\n# === Step 5: Output\nprint(f\"\\n✅ Mapped Mentions ({len(mapped_mentions)}):\")\nfor mention, hpo_id in mapped_mentions:\n    print(f\"{mention} --> {hpo_id}\")\n\nprint(f\"\\n❌ Unmapped Mentions ({len(unmapped_mentions)}):\")\nfor mention, _ in unmapped_mentions:\n    print(mention)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\nOUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\nMAX_LENGTH = 512\nDEVICE     = \"cuda:0\"\n\n# === Utility: 清洗文本 ===\ndef clean_text(text: str) -> str:\n    text = text.replace(\"[UNK]\", \" \")\n    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n    text = re.sub(r\"\\d+%?\", \"\", text)\n    text = text.replace(\"##\", \"\")\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    return text.strip()\n\n# === Utility: 判断噪声 mention ===\ndef is_noise(mention: str) -> bool:\n    mention = mention.strip().lower()\n    if not mention or len(mention) < 3:\n        return True\n    if mention in {\"showed\", \"had\", \"was\", \"were\", \"is\", \"are\", \"and\", \"or\", \"she\", \"he\"}:\n        return True\n    if mention.startswith(\",\") or mention.startswith(\".\") or mention.startswith(\" \"):\n        return True\n    if mention.count(\" \") >= 4:  # 太长的碎片\n        return True\n    if re.fullmatch(r\"[^\\w]+\", mention):\n        return True\n    if re.search(r\"\\b(?:he|she|it|they|this|that)\\b.*\\b(?:is|was|were|had|has|showed)\\b\", mention):\n        return True\n    return False\n\n# === Step 1: 读取数据 ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n\n# === Step 2: 加载模型 ===\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True).to(DEVICE)\nmodel.eval()\nid2label = model.config.id2label\n\n# === Step 3: 加载 HPO 词典 ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph   = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        syn_text = syn.split('\"')[1]\n        hpo_map.setdefault(syn_text.lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower().strip()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=80)\n    return hpo_map[matches[0][0]][0] if matches else None\n\n# === Step 4: 推理并抽取 ===\nnormalized_mentions = []\n\nfor idx, raw in enumerate(orig_sentences):\n    sentence = clean_text(raw)\n    encoding = tokenizer(sentence, return_offsets_mapping=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(DEVICE)\n\n    with torch.no_grad():\n        outputs = model(**{k: encoding[k] for k in [\"input_ids\", \"attention_mask\"]})\n    logits  = outputs.logits[0]\n    preds   = logits.argmax(dim=-1).cpu().tolist()\n    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n\n    span_start = span_end = None\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        if label == \"B-HPO_TERM\":\n            span_start, span_end = offsets[i]\n        elif label == \"I-HPO_TERM\" and span_start is not None:\n            span_end = offsets[i][1]\n        else:\n            if span_start is not None:\n                mention = sentence[span_start:span_end]\n                mention_cleaned = mention.strip()\n                if not is_noise(mention_cleaned):\n                    normalized_mentions.append({\n                        \"sentence_index\": idx,\n                        \"sentence\":       sentence,\n                        \"mention\":        mention_cleaned,\n                        \"span\":           (span_start, span_end),\n                        \"hpo_id\":         normalize_mention(mention_cleaned)\n                    })\n                span_start = span_end = None\n\n    if span_start is not None:\n        mention = sentence[span_start:span_end]\n        mention_cleaned = mention.strip()\n        if not is_noise(mention_cleaned):\n            normalized_mentions.append({\n                \"sentence_index\": idx,\n                \"sentence\":       sentence,\n                \"mention\":        mention_cleaned,\n                \"span\":           (span_start, span_end),\n                \"hpo_id\":         normalize_mention(mention_cleaned)\n            })\n\n# === Step 5: 输出结果 ===\ntotal  = len(normalized_mentions)\nmapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\nprint(f\"Total mentions: {total}\")\nprint(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\nprint(f\"Failed to map:   {total - mapped} ({(total - mapped)/total:.1%})\")\n\nwith OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n    for rec in normalized_mentions:\n        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Results saved to {OUT_FILE.resolve()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nimport obonet\nfrom rapidfuzz import process\nimport torch\n\n# === Config ===\nMODEL_DIR  = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE  = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\nOUT_FILE   = Path(\"/kaggle/working/test_normalized_mentions.jsonl\")\nMAX_LENGTH = 512         # 模型最大 token 长度\nDEVICE     = \"cuda:0\"\n\n# === Utility: 清洗函数（去掉数字、标点、##子词标记、[UNK]） ===\ndef clean_text(text: str) -> str:\n    text = text.replace(\"[UNK]\", \" \")\n    # 去除文献引用 [24, 25]\n    text = re.sub(r\"\\[\\s*\\d+(?:\\s*,\\s*\\d+)*\\s*\\]\", \"\", text)\n    # 去除百分比和孤立数字\n    text = re.sub(r\"\\d+%?\", \"\", text)\n    # 去掉子词标记\n    text = text.replace(\"##\", \"\")\n    # 合并多余空格\n    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n\n# === Utility: 噪声过滤 ===\ndef is_noise(mention: str) -> bool:\n    # 纯数字或百分号\n    if re.fullmatch(r\"\\d+%?\", mention):\n        return True\n    # 纯标点\n    if re.fullmatch(r\"[^\\w\\s]+\", mention):\n        return True\n    # 过短\n    if len(mention.strip()) < 3:\n        return True\n    return False\n\n# === Step 1: 读取测试数据 ===\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [\" \".join(ex[\"tokens\"]) for ex in test_data]\n\n# === Step 2: 加载 tokenizer & model ===\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_DIR, use_fast=True, local_files_only=True\n)\ntokenizer.model_max_length = MAX_LENGTH\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_DIR, local_files_only=True\n).to(DEVICE)\nmodel.eval()\n\n# id -> label 映射，如 \"B-HPO_TERM\", \"I-HPO_TERM\", \"O\"\nid2label = model.config.id2label\n\n# === Step 3: 构建 HPO ontology 映射 ===\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph   = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        text = syn.split('\"')[1]\n        hpo_map.setdefault(text.lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    # 尝试模糊匹配\n    matches = process.extract(key, list(hpo_map.keys()), limit=1, score_cutoff=85)\n    if matches:\n        return hpo_map[matches[0][0]][0]\n    return None\n\n# === Step 4: 对每一句文本清洗 + NER 抽取 ===\nnormalized_mentions = []\n\nfor idx, orig in enumerate(orig_sentences):\n    # 4.1 清洗\n    sentence = clean_text(orig)\n\n    # 4.2 分词 & 截断 & 编码\n    encoding = tokenizer(\n        sentence,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=MAX_LENGTH,\n        return_tensors=\"pt\"\n    ).to(DEVICE)\n\n    # 4.3 模型推理\n    with torch.no_grad():\n        outputs = model(\n            input_ids=encoding[\"input_ids\"],\n            attention_mask=encoding[\"attention_mask\"]\n        )\n    logits  = outputs.logits[0]  # [seq_len, num_labels]\n    preds   = logits.argmax(dim=-1).cpu().tolist()\n    offsets = encoding[\"offset_mapping\"][0].cpu().tolist()\n\n    # 4.4 提取连续 HPO_TERM span\n    span_start = span_end = None\n    for i, label_id in enumerate(preds):\n        label = id2label[label_id]\n        if label == \"B-HPO_TERM\":\n            span_start, span_end = offsets[i]\n        elif label == \"I-HPO_TERM\" and span_start is not None:\n            span_end = offsets[i][1]\n        else:\n            if span_start is not None:\n                mention = sentence[span_start:span_end]\n                hpo_id  = normalize_mention(mention)\n                # 4.5 后处理：只保留有效映射且非噪声的 mention\n                if hpo_id and not is_noise(mention):\n                    normalized_mentions.append({\n                        \"sentence_index\": idx,\n                        \"sentence\":       sentence,\n                        \"mention\":        mention,\n                        \"span\":           (span_start, span_end),\n                        \"hpo_id\":         hpo_id\n                    })\n                span_start = span_end = None\n\n    # 若句尾有未闭合 span\n    if span_start is not None:\n        mention = sentence[span_start:span_end]\n        hpo_id  = normalize_mention(mention)\n        if hpo_id and not is_noise(mention):\n            normalized_mentions.append({\n                \"sentence_index\": idx,\n                \"sentence\":       sentence,\n                \"mention\":        mention,\n                \"span\":           (span_start, span_end),\n                \"hpo_id\":         hpo_id\n            })\n\n# === Step 5: 输出统计 & 保存 ===\ntotal  = len(normalized_mentions)\nmapped = sum(1 for r in normalized_mentions if r[\"hpo_id\"] is not None)\nprint(f\"Total mentions: {total}\")\nprint(f\"Mapped to HP ID: {mapped} ({mapped/total:.1%})\")\nprint(f\"Failed to map:   {total-mapped} ({(total-mapped)/total:.1%})\")\n\nwith OUT_FILE.open(\"w\", encoding=\"utf-8\") as fout:\n    for rec in normalized_mentions:\n        fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Results saved to {OUT_FILE.resolve()}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"8888888888888888888","metadata":{}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\n# -------------------\n# Constants & Paths\n# -------------------\nFILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\nDIR_SILVER  = Path(\"/kaggle/input/hpo-only\")\nOUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_FILE = OUT_DIR / \"train.jsonl\"\nDEV_FILE   = OUT_DIR / \"dev.jsonl\"\nTEST_FILE  = OUT_DIR / \"test.jsonl\"\n\nENTITY_TYPES = {\n    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n}\n\n# -------------------\n# Utility Functions\n# -------------------\ndef iter_jsonl(path: Path):\n    with path.open(\"r\", encoding=\"utf-8\") as fh:\n        for line in fh:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                yield json.loads(line)\n            except json.JSONDecodeError:\n                continue\n\ndef filter_valid_entities(rec):\n    \"\"\"保留有效实体类型，清理无关内容\"\"\"\n    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n    if spans:\n        return {\n            \"text\": rec[\"text\"],\n            \"spans\": spans\n        }\n    return None\n\ndef dump_jsonl(path: Path, data):\n    with path.open(\"w\", encoding=\"utf-8\") as fh:\n        for obj in data:\n            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n\ndef load_filtered_silver(path: Path):\n    extra = []\n    for rec in iter_jsonl(path):\n        rec = filter_valid_entities(rec)\n        if rec:\n            extra.append(rec)\n    return extra\n\n# -------------------\n# Step 1: Load and convert gold data\n# -------------------\nprint(\">> Loading gold data …\")\nmerged_filtered = []\nfor rec in iter_jsonl(FILE_MERGED):\n    filtered = filter_valid_entities(rec)\n    if filtered:\n        merged_filtered.append(filtered)\nprint(f\"Total valid records in gold: {len(merged_filtered)}\")\n\n# -------------------\n# Step 2: Split gold into train/dev/test\n# -------------------\ntrain_dev, test_set = train_test_split(\n    merged_filtered,\n    test_size=0.20,\n    random_state=42\n)\ntrain_set, dev_set = train_test_split(\n    train_dev,\n    test_size=0.25,\n    random_state=42\n)\nprint(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n\n# -------------------\n# Step 3: Add silver data to train set\n# -------------------\nextra_train = []\nif DIR_SILVER.exists():\n    print(\">> Loading silver data from hpo-only/\")\n    for jf in sorted(DIR_SILVER.glob(\"*.jsonl\")):\n        print(f\"  - {jf.name}\")\n        extra_train.extend(load_filtered_silver(jf))\nelse:\n    print(\">> Silver data directory not found.\")\n\ntrain_final = train_set + extra_train\nprint(f\"Final train size: {len(train_final)} (including {len(extra_train)} silver records)\")\n\n# -------------------\n# Step 4: Save to disk\n# -------------------\ndump_jsonl(TRAIN_FILE, train_final)\ndump_jsonl(DEV_FILE, dev_set)\ndump_jsonl(TEST_FILE, test_set)\n\nprint(f\"\\nSaved to:\")\nprint(f\"  ➜ {TRAIN_FILE}\")\nprint(f\"  ➜ {DEV_FILE}\")\nprint(f\"  ➜ {TEST_FILE}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    DataCollatorForTokenClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport evaluate\n\n# === 1. Load pre-split data with silver already included ===\nBIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n\ndef load_jsonl(path: Path):\n    with path.open(encoding=\"utf-8\") as f:\n        return [json.loads(line) for line in f if line.strip()]\n\ntrain_data = load_jsonl(BIO_DIR / \"train.jsonl\")\ndev_data   = load_jsonl(BIO_DIR / \"dev.jsonl\")\ntest_data  = load_jsonl(BIO_DIR / \"test.jsonl\")\n\nds_raw = DatasetDict({\n    \"train\": Dataset.from_list(train_data),\n    \"validation\": Dataset.from_list(dev_data),\n    \"test\": Dataset.from_list(test_data),\n})\nprint(\"✅ Loaded dataset sizes:\", {k: len(v) for k, v in ds_raw.items()})\n\n# === 2. Tokenizer and label mappings ===\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    use_fast=True\n)\n\nlabel_list = [\"O\", \"B-HPO_TERM\", \"I-HPO_TERM\"]\nlabel2id = {label: idx for idx, label in enumerate(label_list)}\nid2label = {idx: label for label, idx in label2id.items()}\n\n# === 3. Span-to-token label encoder ===\ndef encode_and_align_labels(example):\n    text = example[\"text\"]\n    spans = example[\"spans\"]\n    entities = [(s[\"start\"], s[\"end\"]) for s in spans]\n\n    encoding = tokenizer(\n        text,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=512,\n    )\n\n    labels = []\n    for offset in encoding[\"offset_mapping\"]:\n        if offset == (0, 0):\n            labels.append(\"O\")\n            continue\n        tag = \"O\"\n        for start, end in entities:\n            if offset[0] >= start and offset[1] <= end:\n                tag = \"B-HPO_TERM\" if offset[0] == start else \"I-HPO_TERM\"\n                break\n        labels.append(tag)\n\n    encoding[\"labels\"] = [label2id[l] for l in labels]\n    return encoding\n\n# === 4. Encode all splits ===\nds_encoded = ds_raw.map(\n    encode_and_align_labels,\n    batched=False,\n    remove_columns=[\"text\", \"spans\"]\n)\nprint(\"✅ Encoding complete.\")\n\n# === 5. Load model ===\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    num_labels=len(label_list),\n    id2label=id2label,\n    label2id=label2id,\n)\n\n# === 6. Evaluation metrics ===\nseqeval = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    labels = p.label_ids\n    true_labels = [\n        [id2label[lid] for lid in seq if lid != -100]\n        for seq in labels\n    ]\n    pred_labels = [\n        [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n        for pred_seq, label_seq in zip(preds, labels)\n    ]\n    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n    return {\n        \"overall_precision\": result[\"overall_precision\"],\n        \"overall_recall\":    result[\"overall_recall\"],\n        \"overall_f1\":        result[\"overall_f1\"],\n        \"overall_accuracy\":  result[\"overall_accuracy\"],\n    }\n\n# === 7. Training configuration ===\ntraining_args = TrainingArguments(\n    output_dir=\"ner_pubmedbert\",\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"overall_f1\",\n    greater_is_better=True,\n    report_to=[\"none\"],\n    save_total_limit=1,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_encoded[\"train\"],\n    eval_dataset=ds_encoded[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForTokenClassification(tokenizer),\n    compute_metrics=compute_metrics,\n)\n\n# === 8. Train and evaluate ===\ntrainer.train()\ntrainer.evaluate()\n\n# === 9. Predict on test set ===\nprint(\"\\n--- Predicting on test set ---\")\npred_output = trainer.predict(ds_encoded[\"test\"])\npreds = pred_output.predictions.argmax(-1)\nlabels = pred_output.label_ids\n\ntrue_labels = [\n    [id2label[lid] for lid in seq if lid != -100]\n    for seq in labels\n]\npred_labels = [\n    [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n    for pred_seq, label_seq in zip(preds, labels)\n]\n\ndetailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n\nprint(\"\\n📊 HPO_TERM classification report:\")\nfor label, metrics in detailed_result.items():\n    if label == \"HPO_TERM\":\n        print(f\"{label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n\n# === Config ===\nMODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE = Path(\"/kaggle/working/bio_outputs/test.jsonl\")  # 使用原始text字段的测试集\nMAX_LENGTH = 512\nDEVICE = 0  # use -1 for CPU (if no GPU)\n\n# === Step 1: Load test data ===\nprint(\">> Loading test data\")\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [ex[\"text\"] for ex in test_data]\n\n# === Step 2: Load model and tokenizer with pipeline ===\nprint(\">> Loading model and tokenizer\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n\nner_pipeline = pipeline(\n    \"ner\",\n    model=model,\n    tokenizer=tokenizer,\n    aggregation_strategy=\"simple\",  # 拼接B/I标签\n    device=DEVICE\n)\n\n# === Step 3: Run NER without post-processing\nprint(\">> Running NER without post-processing\")\nall_results = []\n\nfor idx, sentence in enumerate(orig_sentences):\n    results = ner_pipeline(sentence)\n    for ent in results:\n        word = ent[\"word\"]\n        start = ent[\"start\"]\n        end = ent[\"end\"]\n        label = ent[\"entity_group\"]\n        score = ent[\"score\"]\n        all_results.append({\n            \"sentence_idx\": idx,\n            \"text\": sentence,\n            \"mention\": word,\n            \"start\": start,\n            \"end\": end,\n            \"label\": label,\n            \"score\": round(score, 4)\n        })\n\n# === Step 4: Print Results\nprint(f\"\\nTotal mentions extracted: {len(all_results)}\")\nfor r in all_results:\n    print(f\"[{r['label']}] {r['mention']} (score={r['score']}, span={r['start']}-{r['end']})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport json\nfrom pathlib import Path\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\nimport obonet\nfrom rapidfuzz import process\n\n# === Config ===\nMODEL_DIR = \"/kaggle/working/ner_pubmedbert_saved_HPO\"\nTEST_FILE = Path(\"/kaggle/working/bio_outputs/test.jsonl\")\nMAX_LENGTH = 512\nDEVICE = 0  # use -1 for CPU\n\n# === Step 1: Load test data ===\nprint(\">> Loading test data\")\ntest_data = [json.loads(line) for line in TEST_FILE.open(encoding=\"utf-8\")]\norig_sentences = [ex[\"text\"] for ex in test_data]\n\n# === Step 2: Load model and tokenizer with pipeline ===\nprint(\">> Loading model and tokenizer\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True, local_files_only=True)\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n\nner_pipeline = pipeline(\n    \"ner\",\n    model=model,\n    tokenizer=tokenizer,\n    aggregation_strategy=\"simple\",\n    device=DEVICE\n)\n\n# === Step 3: Load HPO terms from hp.obo\nprint(\">> Loading HPO terms from obo\")\nobo_url = \"http://purl.obolibrary.org/obo/hp.obo\"\ngraph = obonet.read_obo(obo_url)\nhpo_map = {}\nfor node_id, data in graph.nodes(data=True):\n    name = data.get(\"name\")\n    if name:\n        hpo_map.setdefault(name.lower(), []).append(node_id)\n    for syn in data.get(\"synonym\", []):\n        match = re.search(r'\"(.+?)\"', syn)\n        if match:\n            hpo_map.setdefault(match.group(1).lower(), []).append(node_id)\n\ndef normalize_mention(text: str):\n    key = text.lower()\n    if key in hpo_map:\n        return hpo_map[key][0]\n    match = process.extractOne(key, hpo_map.keys(), score_cutoff=85)\n    if match:\n        return hpo_map[match[0]][0]\n    return None\n\n# === Step 4: Run NER + Normalize (No Noise Filtering)\nprint(\">> Running NER and normalization (no filtering)\")\nmapped_mentions = []\nunmapped_mentions = []\n\nfor idx, sentence in enumerate(orig_sentences):\n    results = ner_pipeline(sentence)\n    for ent in results:\n        mention = ent[\"word\"].strip()\n        hpo_id = normalize_mention(mention)\n        (mapped_mentions if hpo_id else unmapped_mentions).append((mention, hpo_id))\n\n# === Step 5: Output\nprint(f\"\\n✅ Mapped Mentions ({len(mapped_mentions)}):\")\nfor mention, hpo_id in mapped_mentions:\n    print(f\"{mention} --> {hpo_id}\")\n\nprint(f\"\\n❌ Unmapped Mentions ({len(unmapped_mentions)}):\")\nfor mention, _ in unmapped_mentions:\n    print(mention)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\n\n# -------------------\n# Constants & Paths\n# -------------------\nFILE_MERGED = Path(\"/kaggle/working/merged_spans_with_entities.jsonl\")\nSILVER_FILE = Path(\"/kaggle/input/hpo-only/HPO_only.jsonl\")\nOUT_DIR     = Path(\"/kaggle/working/bio_outputs\")\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\nTRAIN_FILE = OUT_DIR / \"train.jsonl\"\nDEV_FILE   = OUT_DIR / \"dev.jsonl\"\nTEST_FILE  = OUT_DIR / \"test.jsonl\"\n\nENTITY_TYPES = {\n    \"AGE_ONSET\", \"AGE_FOLLOWUP\", \"AGE_DEATH\",\n    \"PATIENT\", \"HPO_TERM\", \"GENE\", \"GENE_VARIANT\"\n}\n\n# -------------------\n# Utility Functions\n# -------------------\ndef iter_jsonl(path: Path):\n    with path.open(\"r\", encoding=\"utf-8\") as fh:\n        for line in fh:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                yield json.loads(line)\n            except json.JSONDecodeError:\n                continue\n\ndef filter_valid_entities(rec):\n    \"\"\"保留有效实体类型，清理无关内容\"\"\"\n    spans = [s for s in rec.get(\"spans\", []) if s.get(\"label\") in ENTITY_TYPES]\n    if spans:\n        return {\n            \"text\": rec[\"text\"],\n            \"spans\": spans\n        }\n    return None\n\ndef dump_jsonl(path: Path, data):\n    with path.open(\"w\", encoding=\"utf-8\") as fh:\n        for obj in data:\n            fh.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n\n# -------------------\n# Step 1: Load and convert gold data\n# -------------------\nprint(\">> Loading gold data …\")\nmerged_filtered = []\nfor rec in iter_jsonl(FILE_MERGED):\n    filtered = filter_valid_entities(rec)\n    if filtered:\n        merged_filtered.append(filtered)\nprint(f\"Total valid records in gold: {len(merged_filtered)}\")\n\n# -------------------\n# Step 2: Split gold into train/dev/test\n# -------------------\ntrain_dev, test_set = train_test_split(\n    merged_filtered,\n    test_size=0.20,\n    random_state=42\n)\ntrain_set, dev_set = train_test_split(\n    train_dev,\n    test_size=0.25,\n    random_state=42\n)\nprint(f\"Split sizes – TRAIN: {len(train_set)}, DEV: {len(dev_set)}, TEST: {len(test_set)}\")\n\n# -------------------\n# Step 3: Add silver data to train set\n# -------------------\nextra_train = []\nif SILVER_FILE.exists():\n    print(\">> Loading silver data from HPO_only.jsonl\")\n    for rec in iter_jsonl(SILVER_FILE):\n        filtered = filter_valid_entities(rec)\n        if filtered:\n            extra_train.append(filtered)\n    print(f\"  ➜ Loaded {len(extra_train)} silver records.\")\nelse:\n    print(f\">> Silver file not found: {SILVER_FILE}\")\n\ntrain_final = train_set + extra_train\nprint(f\"Final train size: {len(train_final)} (including {len(extra_train)} silver records)\")\n\n# -------------------\n# Step 4: Save to disk\n# -------------------\ndump_jsonl(TRAIN_FILE, train_final)\ndump_jsonl(DEV_FILE, dev_set)\ndump_jsonl(TEST_FILE, test_set)\n\nprint(f\"\\nSaved to:\")\nprint(f\"  ➜ {TRAIN_FILE}\")\nprint(f\"  ➜ {DEV_FILE}\")\nprint(f\"  ➜ {TEST_FILE}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom pathlib import Path\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    DataCollatorForTokenClassification,\n    TrainingArguments,\n    Trainer,\n)\nimport evaluate\n\n# === 1. Load pre-split data with silver already included ===\nBIO_DIR = Path(\"/kaggle/working/bio_outputs\")\n\ndef load_jsonl(path: Path):\n    with path.open(encoding=\"utf-8\") as f:\n        return [json.loads(line) for line in f if line.strip()]\n\ntrain_data = load_jsonl(BIO_DIR / \"train.jsonl\")\ndev_data   = load_jsonl(BIO_DIR / \"dev.jsonl\")\ntest_data  = load_jsonl(BIO_DIR / \"test.jsonl\")\n\nds_raw = DatasetDict({\n    \"train\": Dataset.from_list(train_data),\n    \"validation\": Dataset.from_list(dev_data),\n    \"test\": Dataset.from_list(test_data),\n})\nprint(\"✅ Loaded dataset sizes:\", {k: len(v) for k, v in ds_raw.items()})\n\n# === 2. Tokenizer and label mappings ===\ntokenizer = AutoTokenizer.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    use_fast=True\n)\n\nlabel_list = [\"O\", \"B-HPO_TERM\", \"I-HPO_TERM\"]\nlabel2id = {label: idx for idx, label in enumerate(label_list)}\nid2label = {idx: label for label, idx in label2id.items()}\n\n# === 3. Span-to-token label encoder ===\ndef encode_and_align_labels(example):\n    text = example[\"text\"]\n    spans = example[\"spans\"]\n    entities = [(s[\"start\"], s[\"end\"]) for s in spans]\n\n    encoding = tokenizer(\n        text,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=512,\n    )\n\n    labels = []\n    for offset in encoding[\"offset_mapping\"]:\n        if offset == (0, 0):\n            labels.append(\"O\")\n            continue\n        tag = \"O\"\n        for start, end in entities:\n            if offset[0] >= start and offset[1] <= end:\n                tag = \"B-HPO_TERM\" if offset[0] == start else \"I-HPO_TERM\"\n                break\n        labels.append(tag)\n\n    encoding[\"labels\"] = [label2id[l] for l in labels]\n    return encoding\n\n# === 4. Encode all splits ===\nds_encoded = ds_raw.map(\n    encode_and_align_labels,\n    batched=False,\n    remove_columns=[\"text\", \"spans\"]\n)\nprint(\"✅ Encoding complete.\")\n\n# === 5. Load model ===\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\",\n    num_labels=len(label_list),\n    id2label=id2label,\n    label2id=label2id,\n)\n\n# === 6. Evaluation metrics ===\nseqeval = evaluate.load(\"seqeval\")\n\ndef compute_metrics(p):\n    preds = p.predictions.argmax(-1)\n    labels = p.label_ids\n    true_labels = [\n        [id2label[lid] for lid in seq if lid != -100]\n        for seq in labels\n    ]\n    pred_labels = [\n        [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n        for pred_seq, label_seq in zip(preds, labels)\n    ]\n    result = seqeval.compute(predictions=pred_labels, references=true_labels)\n    return {\n        \"overall_precision\": result[\"overall_precision\"],\n        \"overall_recall\":    result[\"overall_recall\"],\n        \"overall_f1\":        result[\"overall_f1\"],\n        \"overall_accuracy\":  result[\"overall_accuracy\"],\n    }\n\n# === 7. Training configuration ===\ntraining_args = TrainingArguments(\n    output_dir=\"ner_pubmedbert\",\n    eval_strategy=\"steps\",\n    eval_steps=50,\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    learning_rate=3e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"overall_f1\",\n    greater_is_better=True,\n    report_to=[\"none\"],\n    save_total_limit=1,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_encoded[\"train\"],\n    eval_dataset=ds_encoded[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForTokenClassification(tokenizer),\n    compute_metrics=compute_metrics,\n)\n\n# === 8. Train and evaluate ===\ntrainer.train()\ntrainer.evaluate()\n\n# === 9. Predict on test set ===\nprint(\"\\n--- Predicting on test set ---\")\npred_output = trainer.predict(ds_encoded[\"test\"])\npreds = pred_output.predictions.argmax(-1)\nlabels = pred_output.label_ids\n\ntrue_labels = [\n    [id2label[lid] for lid in seq if lid != -100]\n    for seq in labels\n]\npred_labels = [\n    [id2label[pid] for pid, lid in zip(pred_seq, label_seq) if lid != -100]\n    for pred_seq, label_seq in zip(preds, labels)\n]\n\ndetailed_result = seqeval.compute(predictions=pred_labels, references=true_labels)\n\nprint(\"\\n📊 HPO_TERM classification report:\")\nfor label, metrics in detailed_result.items():\n    if label == \"HPO_TERM\":\n        print(f\"{label:20} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}